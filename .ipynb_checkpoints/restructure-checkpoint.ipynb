{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# nn.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This script trains a deep learning model to predict a target variable based on input features.\n",
    "\n",
    "- The input features and target variable are read from CSV files.\n",
    "- The model is a feed-forward neural network with batch normalization and dropout layers.\n",
    "- The model is trained using Stochastic Gradient Descent (SGD) with a learning rate of 0.001.\n",
    "- The loss function is Mean Squared Error (MSE).\n",
    "- The model's performance is evaluated on a separate test dataset after each training epoch.\n",
    "- Training and test losses are printed for each epoch.\n",
    "- The trained model parameters are saved after each epoch.\n",
    "- The script supports training on multiple GPUs if they are available.\n",
    "\"\"\"\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import pandas as pd\n",
    "import os\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=pd.errors.SettingWithCopyWarning)\n",
    "from functions import FullConnectedBlock, NeuralNetwork2\n",
    "\n",
    "\n",
    "\n",
    "############ Load Training Data ############################\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print('Device:', device)\n",
    "print(f'Number of available GPUs: {torch.cuda.device_count()}')\n",
    "\n",
    "data1 = pd.read_csv(\"/mnt/data/macaulay/datas/training_omicExpression_Embeddings.csv\")\n",
    "data2 = pd.read_csv(\"/mnt/data/macaulay/datas/training_gene_embeddings.csv\")\n",
    "df_Y = pd.read_csv('/mnt/data/macaulay/datas/training_crispr.csv')\n",
    "print('Training data loaded successfully')\n",
    "\n",
    "batch_size1 = 30\n",
    "avg_train_losses = []\n",
    "combined_batches = []\n",
    "input_dim = data1.shape[1] + data2.shape[1] - 1\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "\n",
    "\n",
    "model = NeuralNetwork2(input_dim)\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001)\n",
    "\n",
    "if torch.cuda.device_count() >= 4:\n",
    "    print(\"Using 4 GPUs!\")\n",
    "    model = nn.DataParallel(model, device_ids=list(range(4)))\n",
    "\n",
    "print('Model initialized successfully')\n",
    "saved_models = os.listdir('/mnt/data/macaulay/model_state2')\n",
    "\n",
    "\n",
    "epochs = [int(file.split('_')[-1].split('.')[0]) for file in saved_models if 'crispr_fc1_model_state_epoch_' in file]\n",
    "\n",
    "\n",
    "last_epoch = max(epochs) if epochs else 0\n",
    "start_epoch = last_epoch + 1\n",
    "end_epoch = start_epoch + 20\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for epoch in range(start_epoch, end_epoch):\n",
    "    # ... training loop ...\n",
    "\n",
    "    print(f\"Starting New Epoch\")\n",
    "\n",
    "    Y_data_loader = 0\n",
    "    epoch_loss = 0\n",
    "    model_path = f'/mnt/data/macaulay/model_state2/crispr_fc1_model_state_epoch_{epoch-1}.pth'\n",
    "    model.to(device)\n",
    "    if os.path.exists(model_path):\n",
    "\n",
    "        model.load_state_dict(torch.load(model_path))\n",
    "        print(f'Model {epoch - 1} loaded successfully for epoch {epoch}')\n",
    "\n",
    "    else:\n",
    "        print('No saved model found. Training from scratch.')\n",
    "\n",
    "    for i in range(0, len(data1), batch_size1):\n",
    "        batch_data1 = data1.iloc[i:i + batch_size1]\n",
    "        batch_data2 = data2.iloc[0:len(data2)]\n",
    "\n",
    "\n",
    "        batch_data1['key'] = 1\n",
    "        batch_data2['key'] = 1\n",
    "        batch_data1 = batch_data1[list(data1.columns[0:]) + ['key']]\n",
    "        batch_data2 = batch_data2[list(data2.columns[1:]) + ['key']]\n",
    "        combined_batch = pd.merge(batch_data1, batch_data2, on='key').drop(columns=['key'])\n",
    "        combined_batches.append(combined_batch)\n",
    "\n",
    "\n",
    "        X_train = pd.concat(combined_batches)\n",
    "        Y_train = df_Y.iloc[Y_data_loader:(Y_data_loader + (len(data2) * batch_size1))]\n",
    "        Y_data_loader += (len(data2) * batch_size1)\n",
    "\n",
    "        combined_batches = []\n",
    "\n",
    "\n",
    "        X_train1 = torch.tensor(X_train.values, dtype=torch.float32)\n",
    "        Y_train1 = torch.tensor(Y_train.values.reshape(-1, 1), dtype=torch.float32)  # Reshape Y_train to (num_samples, 1)\n",
    "\n",
    "        train_data = TensorDataset(X_train1, Y_train1)\n",
    "        train_dataloader = DataLoader(train_data, batch_size=128, shuffle=True, num_workers=4, pin_memory=True)\n",
    "\n",
    "        train_loss = 0.0\n",
    "        model.train()\n",
    "\n",
    "        for batch, (inputs, targets) in enumerate(train_dataloader):\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = loss_fn(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = train_loss / len(train_dataloader)\n",
    "\n",
    "        print(f\"Batch {i//batch_size1 + 1}.{epoch}: Avg. training loss = {avg_train_loss:.4f}\")\n",
    "        epoch_loss += avg_train_loss\n",
    "\n",
    "        print(f\"{i+batch_size1} of cell line is done\")\n",
    "\n",
    "    avg_epoch_loss = epoch_loss / ((len(data1) // batch_size1) * (len(data2) // len(data2)))\n",
    "    avg_train_losses.append(avg_epoch_loss)\n",
    "\n",
    "    torch.save(model.state_dict(), f'/mnt/data/macaulay/model_state2/crispr_fc1_model_state_epoch_{epoch}.pth')\n",
    "    print('model saved successfully')\n",
    "    print(f\"Epoch {epoch}/{end_epoch-1}: Avg. epoch loss = {avg_epoch_loss:.4f}\")\n",
    "\n",
    "\n",
    "loss_df = pd.DataFrame(avg_train_losses, columns=[\"Avg_Train_Loss\"])\n",
    "loss_filepath = \"datas/crispr_training_loss.csv\"\n",
    "loss_df.to_csv(loss_filepath, index=False)\n",
    "print(f\"Training loss saved to {loss_filepath}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import pandas as pd\n",
    "import os\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=pd.errors.SettingWithCopyWarning)\n",
    "from functions import FullConnectedBlock, NeuralNetwork2\n",
    "\n",
    "\n",
    "\n",
    "def initialize_environment(data1_path, data2_path, df_Y_path, learning_rate):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print('Device:', device)\n",
    "    print(f'Number of available GPUs: {torch.cuda.device_count()}')\n",
    "\n",
    "    data1 = pd.read_csv(data1_path)\n",
    "    data2 = pd.read_csv(data2_path)\n",
    "    df_Y = pd.read_csv(df_Y_path)\n",
    "    print('Training data loaded successfully')\n",
    "\n",
    "    loss_fn = nn.MSELoss()\n",
    "    input_dim = data1.shape[1] + data2.shape[1] - 1\n",
    "    model = NeuralNetwork2(input_dim)\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    if torch.cuda.device_count() >= 4:\n",
    "        print(\"Using 4 GPUs!\")\n",
    "        model = nn.DataParallel(model, device_ids=list(range(4)))\n",
    "\n",
    "    print('Model initialized successfully')\n",
    "    \n",
    "    return device, data1, data2, df_Y, model, optimizer, loss_fn\n",
    "\n",
    "def cartesian_product(data1, data2):\n",
    "    data1 = data1.iloc[:, 1:]\n",
    "    data1['key'] = 1\n",
    "    data2['key'] = 1\n",
    "    combined_data = pd.merge(data1, data2, on='key').drop(columns=['key'])\n",
    "    return combined_data\n",
    "\n",
    "def cartesian_product_generator(data1, data2, df_Y, batch_size1):\n",
    "    for i in range(0, len(data1), batch_size1):\n",
    "        start_idx = i * len(data2)\n",
    "        end_idx = (i + batch_size1) * len(data2)\n",
    "        \n",
    "        batch_data1 = data1.iloc[i:i + batch_size1]\n",
    "        combined_data = cartesian_product(batch_data1, data2)\n",
    "        \n",
    "        batch_Y = df_Y.iloc[start_idx:end_idx]\n",
    "        \n",
    "        yield combined_data, batch_Y\n",
    "\n",
    "def load_model(model, epoch, model_save_path):\n",
    "    model_path = os.path.join(model_save_path, f'crispr_fc1_model_state_epoch_{epoch-1}.pth')\n",
    "    if os.path.exists(model_path):\n",
    "        model.load_state_dict(torch.load(model_path))\n",
    "        print(f'Model {epoch - 1} loaded successfully for epoch {epoch}')\n",
    "    else:\n",
    "        print('No saved model found. Training from scratch.')\n",
    "    return model\n",
    "\n",
    "# Main Training Loop\n",
    "\n",
    "def main_training_loop(data1_path, data2_path, df_Y_path, batch_size1, learning_rate, num_epochs, model_save_path):\n",
    "    \n",
    "    \n",
    "    device, data1, data2, df_Y, model, optimizer, loss_fn = initialize_environment(data1_path, data2_path, df_Y_path, learning_rate)\n",
    "    \n",
    "    saved_models = os.listdir(model_save_path)\n",
    "    epochs = [int(file.split('_')[-1].split('.')[0]) for file in saved_models if 'crispr_fc1_model_state_epoch_' in file]\n",
    "    last_epoch = max(epochs) if epochs else 0\n",
    "    start_epoch = last_epoch + 1\n",
    "    end_epoch = start_epoch + num_epochs\n",
    "    \n",
    "    all_train_losses = []\n",
    "\n",
    "    for epoch in range(start_epoch, end_epoch):\n",
    "        print(f\"Starting New Epoch {epoch}\")\n",
    "        model = load_model(model, epoch, model_save_path)\n",
    "        \n",
    "        for batch_X, batch_Y in cartesian_product_generator(data1, data2, df_Y, batch_size1):\n",
    "            X_train = torch.tensor(batch_X.values, dtype=torch.float32)\n",
    "            Y_train = torch.tensor(batch_Y.values.reshape(-1, 1), dtype=torch.float32)\n",
    "            \n",
    "            train_data = TensorDataset(X_train, Y_train)\n",
    "            train_dataloader = DataLoader(train_data, batch_size=128, shuffle=True, num_workers=4, pin_memory=True)\n",
    "\n",
    "            train_loss = 0.0\n",
    "            model.train()\n",
    "            print('training')\n",
    "            for inputs, targets in train_dataloader:\n",
    "                inputs, targets = inputs.to(device), targets.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(inputs)\n",
    "                loss = loss_fn(outputs, targets)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                train_loss += loss.item()\n",
    "\n",
    "            avg_train_loss = train_loss / len(train_dataloader)\n",
    "            all_train_losses.append(avg_train_loss)\n",
    "\n",
    "            print(f\"Epoch {epoch}.{batch_X.index[0]//batch_size1 + 1}: Avg. training loss = {avg_train_loss:.4f}\")\n",
    "\n",
    "        torch.save(model.state_dict(), f'{model_save_path}crispr_fc1_model_state_epoch_{epoch}.pth')\n",
    "        print('Model saved successfully')\n",
    "\n",
    "    loss_df = pd.DataFrame(all_train_losses, columns=[\"Avg_Train_Loss\"])\n",
    "    loss_filepath = os.path.join(model_save_path, \"crispr_training_loss.csv\")\n",
    "    loss_df.to_csv(loss_filepath, index=False)\n",
    "    print(f\"Training loss saved to {loss_filepath}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n",
      "Number of available GPUs: 0\n",
      "Training data loaded successfully\n",
      "Model initialized successfully\n",
      "Starting New Epoch 1\n",
      "No saved model found. Training from scratch.\n",
      "training\n",
      "Epoch 1.1: Avg. training loss = 0.5493\n",
      "training\n",
      "Epoch 1.1: Avg. training loss = 0.6046\n",
      "training\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[56], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m main_training_loop(\n\u001b[1;32m      2\u001b[0m     data1_path\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mdatas/training_gene_embeddings.csv\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m#gene embeddings\u001b[39;49;00m\n\u001b[1;32m      3\u001b[0m     data2_path\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mdatas/training_omicExpression_Embeddings.csv\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m#omic expression embeddings\u001b[39;49;00m\n\u001b[1;32m      4\u001b[0m     df_Y_path\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mdatas/training_crispr.csv\u001b[39;49m\u001b[39m\"\u001b[39;49m, \n\u001b[1;32m      5\u001b[0m     batch_size1 \u001b[39m=\u001b[39;49m \u001b[39m1\u001b[39;49m, \n\u001b[1;32m      6\u001b[0m     learning_rate\u001b[39m=\u001b[39;49m\u001b[39m0.001\u001b[39;49m, \n\u001b[1;32m      7\u001b[0m     num_epochs\u001b[39m=\u001b[39;49m\u001b[39m20\u001b[39;49m, \n\u001b[1;32m      8\u001b[0m     model_save_path\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mdatas/\u001b[39;49m\u001b[39m'\u001b[39;49m\n\u001b[1;32m      9\u001b[0m )\n",
      "Cell \u001b[0;32mIn[55], line 98\u001b[0m, in \u001b[0;36mmain_training_loop\u001b[0;34m(data1_path, data2_path, df_Y_path, batch_size1, learning_rate, num_epochs, model_save_path)\u001b[0m\n\u001b[1;32m     96\u001b[0m outputs \u001b[39m=\u001b[39m model(inputs)\n\u001b[1;32m     97\u001b[0m loss \u001b[39m=\u001b[39m loss_fn(outputs, targets)\n\u001b[0;32m---> 98\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     99\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m    100\u001b[0m train_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mitem()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    489\u001b[0m )\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    202\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "main_training_loop(\n",
    "    data1_path=\"datas/training_gene_embeddings.csv\", #gene embeddings\n",
    "    data2_path=\"datas/training_omicExpression_Embeddings.csv\", #omic expression embeddings\n",
    "    df_Y_path=\"datas/training_crispr.csv\", \n",
    "    batch_size1 = 1, \n",
    "    learning_rate=0.001, \n",
    "    num_epochs=20, \n",
    "    model_save_path='datas/'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cartesian_product_generator(data1, data2, df_Y, batch_size1):\n",
    "    for i in range(0, len(data1), batch_size1):\n",
    "        start_idx = i * len(data2)\n",
    "        end_idx = (i + batch_size1) * len(data2)\n",
    "        \n",
    "        batch_data1 = data1.iloc[i:i + batch_size1]\n",
    "        combined_data = cartesian_product(batch_data1, data2)\n",
    "        \n",
    "        batch_Y = df_Y.iloc[start_idx:end_idx]\n",
    "        \n",
    "        yield combined_data, batch_Y\n",
    "\n",
    "def cartesian_product(data1, data2):\n",
    "    data1 = data1.iloc[:, 1:]\n",
    "    data1['key'] = 1\n",
    "    data2['key'] = 1\n",
    "    combined_data = pd.merge(data1, data2, on='key').drop(columns=['key'])\n",
    "    return combined_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data loaded successfully\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "data2 = pd.read_csv(\"datas/test_omicExpression_Embeddings.csv\")\n",
    "data1 = pd.read_csv(\"datas/training_gene_embeddings.csv\")\n",
    "df_Y = pd.read_csv('datas/A_test_gene__Y_crispr.csv')\n",
    "print('Training data loaded successfully')\n",
    "batch_size1 = 1\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0_x</th>\n",
       "      <th>1_x</th>\n",
       "      <th>2_x</th>\n",
       "      <th>3_x</th>\n",
       "      <th>4_x</th>\n",
       "      <th>5_x</th>\n",
       "      <th>6_x</th>\n",
       "      <th>7_x</th>\n",
       "      <th>8_x</th>\n",
       "      <th>9_x</th>\n",
       "      <th>...</th>\n",
       "      <th>502_y</th>\n",
       "      <th>503_y</th>\n",
       "      <th>504_y</th>\n",
       "      <th>505_y</th>\n",
       "      <th>506_y</th>\n",
       "      <th>507_y</th>\n",
       "      <th>508_y</th>\n",
       "      <th>509_y</th>\n",
       "      <th>510_y</th>\n",
       "      <th>511_y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.013528</td>\n",
       "      <td>-0.073574</td>\n",
       "      <td>0.050425</td>\n",
       "      <td>-0.007168</td>\n",
       "      <td>-0.083504</td>\n",
       "      <td>0.009434</td>\n",
       "      <td>-0.243107</td>\n",
       "      <td>-0.036453</td>\n",
       "      <td>-0.003885</td>\n",
       "      <td>0.214968</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.002187</td>\n",
       "      <td>0.000594</td>\n",
       "      <td>0.000299</td>\n",
       "      <td>-0.000457</td>\n",
       "      <td>-0.000006</td>\n",
       "      <td>-0.000012</td>\n",
       "      <td>-0.000443</td>\n",
       "      <td>0.000373</td>\n",
       "      <td>0.000129</td>\n",
       "      <td>-0.000948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.013528</td>\n",
       "      <td>-0.073574</td>\n",
       "      <td>0.050425</td>\n",
       "      <td>-0.007168</td>\n",
       "      <td>-0.083504</td>\n",
       "      <td>0.009434</td>\n",
       "      <td>-0.243107</td>\n",
       "      <td>-0.036453</td>\n",
       "      <td>-0.003885</td>\n",
       "      <td>0.214968</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000141</td>\n",
       "      <td>-0.000154</td>\n",
       "      <td>0.000114</td>\n",
       "      <td>0.000162</td>\n",
       "      <td>0.000534</td>\n",
       "      <td>0.000236</td>\n",
       "      <td>-0.000229</td>\n",
       "      <td>0.000265</td>\n",
       "      <td>-0.000047</td>\n",
       "      <td>0.000220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.013528</td>\n",
       "      <td>-0.073574</td>\n",
       "      <td>0.050425</td>\n",
       "      <td>-0.007168</td>\n",
       "      <td>-0.083504</td>\n",
       "      <td>0.009434</td>\n",
       "      <td>-0.243107</td>\n",
       "      <td>-0.036453</td>\n",
       "      <td>-0.003885</td>\n",
       "      <td>0.214968</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000143</td>\n",
       "      <td>-0.000160</td>\n",
       "      <td>0.000110</td>\n",
       "      <td>0.000163</td>\n",
       "      <td>0.000534</td>\n",
       "      <td>0.000240</td>\n",
       "      <td>-0.000230</td>\n",
       "      <td>0.000263</td>\n",
       "      <td>-0.000041</td>\n",
       "      <td>0.000224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.013528</td>\n",
       "      <td>-0.073574</td>\n",
       "      <td>0.050425</td>\n",
       "      <td>-0.007168</td>\n",
       "      <td>-0.083504</td>\n",
       "      <td>0.009434</td>\n",
       "      <td>-0.243107</td>\n",
       "      <td>-0.036453</td>\n",
       "      <td>-0.003885</td>\n",
       "      <td>0.214968</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000141</td>\n",
       "      <td>-0.000159</td>\n",
       "      <td>0.000111</td>\n",
       "      <td>0.000160</td>\n",
       "      <td>0.000535</td>\n",
       "      <td>0.000240</td>\n",
       "      <td>-0.000233</td>\n",
       "      <td>0.000262</td>\n",
       "      <td>-0.000046</td>\n",
       "      <td>0.000224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.013528</td>\n",
       "      <td>-0.073574</td>\n",
       "      <td>0.050425</td>\n",
       "      <td>-0.007168</td>\n",
       "      <td>-0.083504</td>\n",
       "      <td>0.009434</td>\n",
       "      <td>-0.243107</td>\n",
       "      <td>-0.036453</td>\n",
       "      <td>-0.003885</td>\n",
       "      <td>0.214968</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000141</td>\n",
       "      <td>-0.000161</td>\n",
       "      <td>0.000113</td>\n",
       "      <td>0.000163</td>\n",
       "      <td>0.000538</td>\n",
       "      <td>0.000240</td>\n",
       "      <td>-0.000231</td>\n",
       "      <td>0.000263</td>\n",
       "      <td>-0.000050</td>\n",
       "      <td>0.000219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.013528</td>\n",
       "      <td>-0.073574</td>\n",
       "      <td>0.050425</td>\n",
       "      <td>-0.007168</td>\n",
       "      <td>-0.083504</td>\n",
       "      <td>0.009434</td>\n",
       "      <td>-0.243107</td>\n",
       "      <td>-0.036453</td>\n",
       "      <td>-0.003885</td>\n",
       "      <td>0.214968</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000143</td>\n",
       "      <td>-0.000161</td>\n",
       "      <td>0.000111</td>\n",
       "      <td>0.000164</td>\n",
       "      <td>0.000532</td>\n",
       "      <td>0.000239</td>\n",
       "      <td>-0.000229</td>\n",
       "      <td>0.000267</td>\n",
       "      <td>-0.000044</td>\n",
       "      <td>0.000221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.013528</td>\n",
       "      <td>-0.073574</td>\n",
       "      <td>0.050425</td>\n",
       "      <td>-0.007168</td>\n",
       "      <td>-0.083504</td>\n",
       "      <td>0.009434</td>\n",
       "      <td>-0.243107</td>\n",
       "      <td>-0.036453</td>\n",
       "      <td>-0.003885</td>\n",
       "      <td>0.214968</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000142</td>\n",
       "      <td>-0.000159</td>\n",
       "      <td>0.000112</td>\n",
       "      <td>0.000163</td>\n",
       "      <td>0.000536</td>\n",
       "      <td>0.000239</td>\n",
       "      <td>-0.000232</td>\n",
       "      <td>0.000264</td>\n",
       "      <td>-0.000046</td>\n",
       "      <td>0.000224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.013528</td>\n",
       "      <td>-0.073574</td>\n",
       "      <td>0.050425</td>\n",
       "      <td>-0.007168</td>\n",
       "      <td>-0.083504</td>\n",
       "      <td>0.009434</td>\n",
       "      <td>-0.243107</td>\n",
       "      <td>-0.036453</td>\n",
       "      <td>-0.003885</td>\n",
       "      <td>0.214968</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000143</td>\n",
       "      <td>-0.000159</td>\n",
       "      <td>0.000111</td>\n",
       "      <td>0.000163</td>\n",
       "      <td>0.000534</td>\n",
       "      <td>0.000235</td>\n",
       "      <td>-0.000234</td>\n",
       "      <td>0.000264</td>\n",
       "      <td>-0.000046</td>\n",
       "      <td>0.000222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.013528</td>\n",
       "      <td>-0.073574</td>\n",
       "      <td>0.050425</td>\n",
       "      <td>-0.007168</td>\n",
       "      <td>-0.083504</td>\n",
       "      <td>0.009434</td>\n",
       "      <td>-0.243107</td>\n",
       "      <td>-0.036453</td>\n",
       "      <td>-0.003885</td>\n",
       "      <td>0.214968</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000140</td>\n",
       "      <td>-0.000158</td>\n",
       "      <td>0.000113</td>\n",
       "      <td>0.000163</td>\n",
       "      <td>0.000537</td>\n",
       "      <td>0.000238</td>\n",
       "      <td>-0.000232</td>\n",
       "      <td>0.000265</td>\n",
       "      <td>-0.000046</td>\n",
       "      <td>0.000225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.013528</td>\n",
       "      <td>-0.073574</td>\n",
       "      <td>0.050425</td>\n",
       "      <td>-0.007168</td>\n",
       "      <td>-0.083504</td>\n",
       "      <td>0.009434</td>\n",
       "      <td>-0.243107</td>\n",
       "      <td>-0.036453</td>\n",
       "      <td>-0.003885</td>\n",
       "      <td>0.214968</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000142</td>\n",
       "      <td>-0.000159</td>\n",
       "      <td>0.000112</td>\n",
       "      <td>0.000164</td>\n",
       "      <td>0.000536</td>\n",
       "      <td>0.000239</td>\n",
       "      <td>-0.000237</td>\n",
       "      <td>0.000266</td>\n",
       "      <td>-0.000047</td>\n",
       "      <td>0.000224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.013528</td>\n",
       "      <td>-0.073574</td>\n",
       "      <td>0.050425</td>\n",
       "      <td>-0.007168</td>\n",
       "      <td>-0.083504</td>\n",
       "      <td>0.009434</td>\n",
       "      <td>-0.243107</td>\n",
       "      <td>-0.036453</td>\n",
       "      <td>-0.003885</td>\n",
       "      <td>0.214968</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000141</td>\n",
       "      <td>-0.000158</td>\n",
       "      <td>0.000112</td>\n",
       "      <td>0.000162</td>\n",
       "      <td>0.000536</td>\n",
       "      <td>0.000238</td>\n",
       "      <td>-0.000233</td>\n",
       "      <td>0.000266</td>\n",
       "      <td>-0.000047</td>\n",
       "      <td>0.000221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.013528</td>\n",
       "      <td>-0.073574</td>\n",
       "      <td>0.050425</td>\n",
       "      <td>-0.007168</td>\n",
       "      <td>-0.083504</td>\n",
       "      <td>0.009434</td>\n",
       "      <td>-0.243107</td>\n",
       "      <td>-0.036453</td>\n",
       "      <td>-0.003885</td>\n",
       "      <td>0.214968</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000139</td>\n",
       "      <td>-0.000159</td>\n",
       "      <td>0.000111</td>\n",
       "      <td>0.000167</td>\n",
       "      <td>0.000532</td>\n",
       "      <td>0.000242</td>\n",
       "      <td>-0.000234</td>\n",
       "      <td>0.000265</td>\n",
       "      <td>-0.000046</td>\n",
       "      <td>0.000220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.013528</td>\n",
       "      <td>-0.073574</td>\n",
       "      <td>0.050425</td>\n",
       "      <td>-0.007168</td>\n",
       "      <td>-0.083504</td>\n",
       "      <td>0.009434</td>\n",
       "      <td>-0.243107</td>\n",
       "      <td>-0.036453</td>\n",
       "      <td>-0.003885</td>\n",
       "      <td>0.214968</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000143</td>\n",
       "      <td>-0.000158</td>\n",
       "      <td>0.000111</td>\n",
       "      <td>0.000163</td>\n",
       "      <td>0.000534</td>\n",
       "      <td>0.000239</td>\n",
       "      <td>-0.000234</td>\n",
       "      <td>0.000265</td>\n",
       "      <td>-0.000045</td>\n",
       "      <td>0.000222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.013528</td>\n",
       "      <td>-0.073574</td>\n",
       "      <td>0.050425</td>\n",
       "      <td>-0.007168</td>\n",
       "      <td>-0.083504</td>\n",
       "      <td>0.009434</td>\n",
       "      <td>-0.243107</td>\n",
       "      <td>-0.036453</td>\n",
       "      <td>-0.003885</td>\n",
       "      <td>0.214968</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000142</td>\n",
       "      <td>-0.000156</td>\n",
       "      <td>0.000111</td>\n",
       "      <td>0.000161</td>\n",
       "      <td>0.000534</td>\n",
       "      <td>0.000238</td>\n",
       "      <td>-0.000232</td>\n",
       "      <td>0.000266</td>\n",
       "      <td>-0.000048</td>\n",
       "      <td>0.000222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.013528</td>\n",
       "      <td>-0.073574</td>\n",
       "      <td>0.050425</td>\n",
       "      <td>-0.007168</td>\n",
       "      <td>-0.083504</td>\n",
       "      <td>0.009434</td>\n",
       "      <td>-0.243107</td>\n",
       "      <td>-0.036453</td>\n",
       "      <td>-0.003885</td>\n",
       "      <td>0.214968</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000142</td>\n",
       "      <td>-0.000160</td>\n",
       "      <td>0.000112</td>\n",
       "      <td>0.000165</td>\n",
       "      <td>0.000535</td>\n",
       "      <td>0.000239</td>\n",
       "      <td>-0.000229</td>\n",
       "      <td>0.000266</td>\n",
       "      <td>-0.000046</td>\n",
       "      <td>0.000223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.013528</td>\n",
       "      <td>-0.073574</td>\n",
       "      <td>0.050425</td>\n",
       "      <td>-0.007168</td>\n",
       "      <td>-0.083504</td>\n",
       "      <td>0.009434</td>\n",
       "      <td>-0.243107</td>\n",
       "      <td>-0.036453</td>\n",
       "      <td>-0.003885</td>\n",
       "      <td>0.214968</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000142</td>\n",
       "      <td>-0.000159</td>\n",
       "      <td>0.000113</td>\n",
       "      <td>0.000161</td>\n",
       "      <td>0.000535</td>\n",
       "      <td>0.000239</td>\n",
       "      <td>-0.000228</td>\n",
       "      <td>0.000264</td>\n",
       "      <td>-0.000048</td>\n",
       "      <td>0.000225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.013528</td>\n",
       "      <td>-0.073574</td>\n",
       "      <td>0.050425</td>\n",
       "      <td>-0.007168</td>\n",
       "      <td>-0.083504</td>\n",
       "      <td>0.009434</td>\n",
       "      <td>-0.243107</td>\n",
       "      <td>-0.036453</td>\n",
       "      <td>-0.003885</td>\n",
       "      <td>0.214968</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000140</td>\n",
       "      <td>0.000144</td>\n",
       "      <td>-0.000165</td>\n",
       "      <td>-0.000127</td>\n",
       "      <td>0.000485</td>\n",
       "      <td>-0.000167</td>\n",
       "      <td>0.000190</td>\n",
       "      <td>-0.000160</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>-0.000167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.013528</td>\n",
       "      <td>-0.073574</td>\n",
       "      <td>0.050425</td>\n",
       "      <td>-0.007168</td>\n",
       "      <td>-0.083504</td>\n",
       "      <td>0.009434</td>\n",
       "      <td>-0.243107</td>\n",
       "      <td>-0.036453</td>\n",
       "      <td>-0.003885</td>\n",
       "      <td>0.214968</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000140</td>\n",
       "      <td>0.000142</td>\n",
       "      <td>-0.000169</td>\n",
       "      <td>-0.000130</td>\n",
       "      <td>0.000488</td>\n",
       "      <td>-0.000166</td>\n",
       "      <td>0.000190</td>\n",
       "      <td>-0.000156</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>-0.000167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.013528</td>\n",
       "      <td>-0.073574</td>\n",
       "      <td>0.050425</td>\n",
       "      <td>-0.007168</td>\n",
       "      <td>-0.083504</td>\n",
       "      <td>0.009434</td>\n",
       "      <td>-0.243107</td>\n",
       "      <td>-0.036453</td>\n",
       "      <td>-0.003885</td>\n",
       "      <td>0.214968</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000141</td>\n",
       "      <td>0.000144</td>\n",
       "      <td>-0.000168</td>\n",
       "      <td>-0.000129</td>\n",
       "      <td>0.000488</td>\n",
       "      <td>-0.000169</td>\n",
       "      <td>0.000190</td>\n",
       "      <td>-0.000159</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>-0.000166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.013528</td>\n",
       "      <td>-0.073574</td>\n",
       "      <td>0.050425</td>\n",
       "      <td>-0.007168</td>\n",
       "      <td>-0.083504</td>\n",
       "      <td>0.009434</td>\n",
       "      <td>-0.243107</td>\n",
       "      <td>-0.036453</td>\n",
       "      <td>-0.003885</td>\n",
       "      <td>0.214968</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000137</td>\n",
       "      <td>0.000144</td>\n",
       "      <td>-0.000164</td>\n",
       "      <td>-0.000127</td>\n",
       "      <td>0.000487</td>\n",
       "      <td>-0.000168</td>\n",
       "      <td>0.000187</td>\n",
       "      <td>-0.000158</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>-0.000167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.013528</td>\n",
       "      <td>-0.073574</td>\n",
       "      <td>0.050425</td>\n",
       "      <td>-0.007168</td>\n",
       "      <td>-0.083504</td>\n",
       "      <td>0.009434</td>\n",
       "      <td>-0.243107</td>\n",
       "      <td>-0.036453</td>\n",
       "      <td>-0.003885</td>\n",
       "      <td>0.214968</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000138</td>\n",
       "      <td>0.000146</td>\n",
       "      <td>-0.000167</td>\n",
       "      <td>-0.000128</td>\n",
       "      <td>0.000486</td>\n",
       "      <td>-0.000168</td>\n",
       "      <td>0.000192</td>\n",
       "      <td>-0.000159</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>-0.000165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.013528</td>\n",
       "      <td>-0.073574</td>\n",
       "      <td>0.050425</td>\n",
       "      <td>-0.007168</td>\n",
       "      <td>-0.083504</td>\n",
       "      <td>0.009434</td>\n",
       "      <td>-0.243107</td>\n",
       "      <td>-0.036453</td>\n",
       "      <td>-0.003885</td>\n",
       "      <td>0.214968</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000140</td>\n",
       "      <td>0.000144</td>\n",
       "      <td>-0.000170</td>\n",
       "      <td>-0.000129</td>\n",
       "      <td>0.000488</td>\n",
       "      <td>-0.000170</td>\n",
       "      <td>0.000187</td>\n",
       "      <td>-0.000160</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>-0.000163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.013528</td>\n",
       "      <td>-0.073574</td>\n",
       "      <td>0.050425</td>\n",
       "      <td>-0.007168</td>\n",
       "      <td>-0.083504</td>\n",
       "      <td>0.009434</td>\n",
       "      <td>-0.243107</td>\n",
       "      <td>-0.036453</td>\n",
       "      <td>-0.003885</td>\n",
       "      <td>0.214968</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000143</td>\n",
       "      <td>0.000147</td>\n",
       "      <td>-0.000168</td>\n",
       "      <td>-0.000131</td>\n",
       "      <td>0.000489</td>\n",
       "      <td>-0.000170</td>\n",
       "      <td>0.000189</td>\n",
       "      <td>-0.000160</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>-0.000165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.013528</td>\n",
       "      <td>-0.073574</td>\n",
       "      <td>0.050425</td>\n",
       "      <td>-0.007168</td>\n",
       "      <td>-0.083504</td>\n",
       "      <td>0.009434</td>\n",
       "      <td>-0.243107</td>\n",
       "      <td>-0.036453</td>\n",
       "      <td>-0.003885</td>\n",
       "      <td>0.214968</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000140</td>\n",
       "      <td>0.000147</td>\n",
       "      <td>-0.000167</td>\n",
       "      <td>-0.000131</td>\n",
       "      <td>0.000489</td>\n",
       "      <td>-0.000169</td>\n",
       "      <td>0.000187</td>\n",
       "      <td>-0.000160</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>-0.000165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.013528</td>\n",
       "      <td>-0.073574</td>\n",
       "      <td>0.050425</td>\n",
       "      <td>-0.007168</td>\n",
       "      <td>-0.083504</td>\n",
       "      <td>0.009434</td>\n",
       "      <td>-0.243107</td>\n",
       "      <td>-0.036453</td>\n",
       "      <td>-0.003885</td>\n",
       "      <td>0.214968</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000139</td>\n",
       "      <td>0.000143</td>\n",
       "      <td>-0.000168</td>\n",
       "      <td>-0.000129</td>\n",
       "      <td>0.000486</td>\n",
       "      <td>-0.000169</td>\n",
       "      <td>0.000191</td>\n",
       "      <td>-0.000159</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>-0.000165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.013528</td>\n",
       "      <td>-0.073574</td>\n",
       "      <td>0.050425</td>\n",
       "      <td>-0.007168</td>\n",
       "      <td>-0.083504</td>\n",
       "      <td>0.009434</td>\n",
       "      <td>-0.243107</td>\n",
       "      <td>-0.036453</td>\n",
       "      <td>-0.003885</td>\n",
       "      <td>0.214968</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000138</td>\n",
       "      <td>0.000144</td>\n",
       "      <td>-0.000165</td>\n",
       "      <td>-0.000126</td>\n",
       "      <td>0.000487</td>\n",
       "      <td>-0.000168</td>\n",
       "      <td>0.000187</td>\n",
       "      <td>-0.000160</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>-0.000166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.013528</td>\n",
       "      <td>-0.073574</td>\n",
       "      <td>0.050425</td>\n",
       "      <td>-0.007168</td>\n",
       "      <td>-0.083504</td>\n",
       "      <td>0.009434</td>\n",
       "      <td>-0.243107</td>\n",
       "      <td>-0.036453</td>\n",
       "      <td>-0.003885</td>\n",
       "      <td>0.214968</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000139</td>\n",
       "      <td>0.000143</td>\n",
       "      <td>-0.000168</td>\n",
       "      <td>-0.000128</td>\n",
       "      <td>0.000489</td>\n",
       "      <td>-0.000169</td>\n",
       "      <td>0.000189</td>\n",
       "      <td>-0.000160</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>-0.000165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.013528</td>\n",
       "      <td>-0.073574</td>\n",
       "      <td>0.050425</td>\n",
       "      <td>-0.007168</td>\n",
       "      <td>-0.083504</td>\n",
       "      <td>0.009434</td>\n",
       "      <td>-0.243107</td>\n",
       "      <td>-0.036453</td>\n",
       "      <td>-0.003885</td>\n",
       "      <td>0.214968</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000137</td>\n",
       "      <td>0.000143</td>\n",
       "      <td>-0.000168</td>\n",
       "      <td>-0.000130</td>\n",
       "      <td>0.000487</td>\n",
       "      <td>-0.000169</td>\n",
       "      <td>0.000187</td>\n",
       "      <td>-0.000160</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>-0.000167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.013528</td>\n",
       "      <td>-0.073574</td>\n",
       "      <td>0.050425</td>\n",
       "      <td>-0.007168</td>\n",
       "      <td>-0.083504</td>\n",
       "      <td>0.009434</td>\n",
       "      <td>-0.243107</td>\n",
       "      <td>-0.036453</td>\n",
       "      <td>-0.003885</td>\n",
       "      <td>0.214968</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007193</td>\n",
       "      <td>-0.001027</td>\n",
       "      <td>-0.000914</td>\n",
       "      <td>0.000910</td>\n",
       "      <td>-0.000024</td>\n",
       "      <td>0.000067</td>\n",
       "      <td>0.001326</td>\n",
       "      <td>-0.000922</td>\n",
       "      <td>-0.000325</td>\n",
       "      <td>0.002181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.013528</td>\n",
       "      <td>-0.073574</td>\n",
       "      <td>0.050425</td>\n",
       "      <td>-0.007168</td>\n",
       "      <td>-0.083504</td>\n",
       "      <td>0.009434</td>\n",
       "      <td>-0.243107</td>\n",
       "      <td>-0.036453</td>\n",
       "      <td>-0.003885</td>\n",
       "      <td>0.214968</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000140</td>\n",
       "      <td>0.000144</td>\n",
       "      <td>-0.000169</td>\n",
       "      <td>-0.000127</td>\n",
       "      <td>0.000488</td>\n",
       "      <td>-0.000170</td>\n",
       "      <td>0.000190</td>\n",
       "      <td>-0.000159</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>-0.000165</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>30 rows Ã— 2412 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0_x       1_x       2_x       3_x       4_x       5_x       6_x  \\\n",
       "0   0.013528 -0.073574  0.050425 -0.007168 -0.083504  0.009434 -0.243107   \n",
       "1   0.013528 -0.073574  0.050425 -0.007168 -0.083504  0.009434 -0.243107   \n",
       "2   0.013528 -0.073574  0.050425 -0.007168 -0.083504  0.009434 -0.243107   \n",
       "3   0.013528 -0.073574  0.050425 -0.007168 -0.083504  0.009434 -0.243107   \n",
       "4   0.013528 -0.073574  0.050425 -0.007168 -0.083504  0.009434 -0.243107   \n",
       "5   0.013528 -0.073574  0.050425 -0.007168 -0.083504  0.009434 -0.243107   \n",
       "6   0.013528 -0.073574  0.050425 -0.007168 -0.083504  0.009434 -0.243107   \n",
       "7   0.013528 -0.073574  0.050425 -0.007168 -0.083504  0.009434 -0.243107   \n",
       "8   0.013528 -0.073574  0.050425 -0.007168 -0.083504  0.009434 -0.243107   \n",
       "9   0.013528 -0.073574  0.050425 -0.007168 -0.083504  0.009434 -0.243107   \n",
       "10  0.013528 -0.073574  0.050425 -0.007168 -0.083504  0.009434 -0.243107   \n",
       "11  0.013528 -0.073574  0.050425 -0.007168 -0.083504  0.009434 -0.243107   \n",
       "12  0.013528 -0.073574  0.050425 -0.007168 -0.083504  0.009434 -0.243107   \n",
       "13  0.013528 -0.073574  0.050425 -0.007168 -0.083504  0.009434 -0.243107   \n",
       "14  0.013528 -0.073574  0.050425 -0.007168 -0.083504  0.009434 -0.243107   \n",
       "15  0.013528 -0.073574  0.050425 -0.007168 -0.083504  0.009434 -0.243107   \n",
       "16  0.013528 -0.073574  0.050425 -0.007168 -0.083504  0.009434 -0.243107   \n",
       "17  0.013528 -0.073574  0.050425 -0.007168 -0.083504  0.009434 -0.243107   \n",
       "18  0.013528 -0.073574  0.050425 -0.007168 -0.083504  0.009434 -0.243107   \n",
       "19  0.013528 -0.073574  0.050425 -0.007168 -0.083504  0.009434 -0.243107   \n",
       "20  0.013528 -0.073574  0.050425 -0.007168 -0.083504  0.009434 -0.243107   \n",
       "21  0.013528 -0.073574  0.050425 -0.007168 -0.083504  0.009434 -0.243107   \n",
       "22  0.013528 -0.073574  0.050425 -0.007168 -0.083504  0.009434 -0.243107   \n",
       "23  0.013528 -0.073574  0.050425 -0.007168 -0.083504  0.009434 -0.243107   \n",
       "24  0.013528 -0.073574  0.050425 -0.007168 -0.083504  0.009434 -0.243107   \n",
       "25  0.013528 -0.073574  0.050425 -0.007168 -0.083504  0.009434 -0.243107   \n",
       "26  0.013528 -0.073574  0.050425 -0.007168 -0.083504  0.009434 -0.243107   \n",
       "27  0.013528 -0.073574  0.050425 -0.007168 -0.083504  0.009434 -0.243107   \n",
       "28  0.013528 -0.073574  0.050425 -0.007168 -0.083504  0.009434 -0.243107   \n",
       "29  0.013528 -0.073574  0.050425 -0.007168 -0.083504  0.009434 -0.243107   \n",
       "\n",
       "         7_x       8_x       9_x  ...     502_y     503_y     504_y     505_y  \\\n",
       "0  -0.036453 -0.003885  0.214968  ... -0.002187  0.000594  0.000299 -0.000457   \n",
       "1  -0.036453 -0.003885  0.214968  ...  0.000141 -0.000154  0.000114  0.000162   \n",
       "2  -0.036453 -0.003885  0.214968  ...  0.000143 -0.000160  0.000110  0.000163   \n",
       "3  -0.036453 -0.003885  0.214968  ...  0.000141 -0.000159  0.000111  0.000160   \n",
       "4  -0.036453 -0.003885  0.214968  ...  0.000141 -0.000161  0.000113  0.000163   \n",
       "5  -0.036453 -0.003885  0.214968  ...  0.000143 -0.000161  0.000111  0.000164   \n",
       "6  -0.036453 -0.003885  0.214968  ...  0.000142 -0.000159  0.000112  0.000163   \n",
       "7  -0.036453 -0.003885  0.214968  ...  0.000143 -0.000159  0.000111  0.000163   \n",
       "8  -0.036453 -0.003885  0.214968  ...  0.000140 -0.000158  0.000113  0.000163   \n",
       "9  -0.036453 -0.003885  0.214968  ...  0.000142 -0.000159  0.000112  0.000164   \n",
       "10 -0.036453 -0.003885  0.214968  ...  0.000141 -0.000158  0.000112  0.000162   \n",
       "11 -0.036453 -0.003885  0.214968  ...  0.000139 -0.000159  0.000111  0.000167   \n",
       "12 -0.036453 -0.003885  0.214968  ...  0.000143 -0.000158  0.000111  0.000163   \n",
       "13 -0.036453 -0.003885  0.214968  ...  0.000142 -0.000156  0.000111  0.000161   \n",
       "14 -0.036453 -0.003885  0.214968  ...  0.000142 -0.000160  0.000112  0.000165   \n",
       "15 -0.036453 -0.003885  0.214968  ...  0.000142 -0.000159  0.000113  0.000161   \n",
       "16 -0.036453 -0.003885  0.214968  ... -0.000140  0.000144 -0.000165 -0.000127   \n",
       "17 -0.036453 -0.003885  0.214968  ... -0.000140  0.000142 -0.000169 -0.000130   \n",
       "18 -0.036453 -0.003885  0.214968  ... -0.000141  0.000144 -0.000168 -0.000129   \n",
       "19 -0.036453 -0.003885  0.214968  ... -0.000137  0.000144 -0.000164 -0.000127   \n",
       "20 -0.036453 -0.003885  0.214968  ... -0.000138  0.000146 -0.000167 -0.000128   \n",
       "21 -0.036453 -0.003885  0.214968  ... -0.000140  0.000144 -0.000170 -0.000129   \n",
       "22 -0.036453 -0.003885  0.214968  ... -0.000143  0.000147 -0.000168 -0.000131   \n",
       "23 -0.036453 -0.003885  0.214968  ... -0.000140  0.000147 -0.000167 -0.000131   \n",
       "24 -0.036453 -0.003885  0.214968  ... -0.000139  0.000143 -0.000168 -0.000129   \n",
       "25 -0.036453 -0.003885  0.214968  ... -0.000138  0.000144 -0.000165 -0.000126   \n",
       "26 -0.036453 -0.003885  0.214968  ... -0.000139  0.000143 -0.000168 -0.000128   \n",
       "27 -0.036453 -0.003885  0.214968  ... -0.000137  0.000143 -0.000168 -0.000130   \n",
       "28 -0.036453 -0.003885  0.214968  ...  0.007193 -0.001027 -0.000914  0.000910   \n",
       "29 -0.036453 -0.003885  0.214968  ... -0.000140  0.000144 -0.000169 -0.000127   \n",
       "\n",
       "       506_y     507_y     508_y     509_y     510_y     511_y  \n",
       "0  -0.000006 -0.000012 -0.000443  0.000373  0.000129 -0.000948  \n",
       "1   0.000534  0.000236 -0.000229  0.000265 -0.000047  0.000220  \n",
       "2   0.000534  0.000240 -0.000230  0.000263 -0.000041  0.000224  \n",
       "3   0.000535  0.000240 -0.000233  0.000262 -0.000046  0.000224  \n",
       "4   0.000538  0.000240 -0.000231  0.000263 -0.000050  0.000219  \n",
       "5   0.000532  0.000239 -0.000229  0.000267 -0.000044  0.000221  \n",
       "6   0.000536  0.000239 -0.000232  0.000264 -0.000046  0.000224  \n",
       "7   0.000534  0.000235 -0.000234  0.000264 -0.000046  0.000222  \n",
       "8   0.000537  0.000238 -0.000232  0.000265 -0.000046  0.000225  \n",
       "9   0.000536  0.000239 -0.000237  0.000266 -0.000047  0.000224  \n",
       "10  0.000536  0.000238 -0.000233  0.000266 -0.000047  0.000221  \n",
       "11  0.000532  0.000242 -0.000234  0.000265 -0.000046  0.000220  \n",
       "12  0.000534  0.000239 -0.000234  0.000265 -0.000045  0.000222  \n",
       "13  0.000534  0.000238 -0.000232  0.000266 -0.000048  0.000222  \n",
       "14  0.000535  0.000239 -0.000229  0.000266 -0.000046  0.000223  \n",
       "15  0.000535  0.000239 -0.000228  0.000264 -0.000048  0.000225  \n",
       "16  0.000485 -0.000167  0.000190 -0.000160  0.000005 -0.000167  \n",
       "17  0.000488 -0.000166  0.000190 -0.000156  0.000008 -0.000167  \n",
       "18  0.000488 -0.000169  0.000190 -0.000159  0.000008 -0.000166  \n",
       "19  0.000487 -0.000168  0.000187 -0.000158  0.000005 -0.000167  \n",
       "20  0.000486 -0.000168  0.000192 -0.000159  0.000008 -0.000165  \n",
       "21  0.000488 -0.000170  0.000187 -0.000160  0.000008 -0.000163  \n",
       "22  0.000489 -0.000170  0.000189 -0.000160  0.000006 -0.000165  \n",
       "23  0.000489 -0.000169  0.000187 -0.000160  0.000005 -0.000165  \n",
       "24  0.000486 -0.000169  0.000191 -0.000159  0.000007 -0.000165  \n",
       "25  0.000487 -0.000168  0.000187 -0.000160  0.000007 -0.000166  \n",
       "26  0.000489 -0.000169  0.000189 -0.000160  0.000004 -0.000165  \n",
       "27  0.000487 -0.000169  0.000187 -0.000160  0.000006 -0.000167  \n",
       "28 -0.000024  0.000067  0.001326 -0.000922 -0.000325  0.002181  \n",
       "29  0.000488 -0.000170  0.000190 -0.000159  0.000006 -0.000165  \n",
       "\n",
       "[30 rows x 2412 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.271543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.567019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.504965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.590148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.281329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-0.421348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-0.107869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-0.586895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-0.442476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-0.211472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>-0.354844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>-0.154453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>-0.441073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>-0.379360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>-0.059150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>-0.227465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>-0.655026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>-0.463215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.152592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>-0.217824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>-0.778586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>-0.527723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>-0.051708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>-0.481998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>-0.228224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>-0.118447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>-0.374483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.034428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>-0.213135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>-0.257388</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Y\n",
       "0   0.271543\n",
       "1  -0.567019\n",
       "2  -0.504965\n",
       "3  -0.590148\n",
       "4  -0.281329\n",
       "5  -0.421348\n",
       "6  -0.107869\n",
       "7  -0.586895\n",
       "8  -0.442476\n",
       "9  -0.211472\n",
       "10 -0.354844\n",
       "11 -0.154453\n",
       "12 -0.441073\n",
       "13 -0.379360\n",
       "14 -0.059150\n",
       "15 -0.227465\n",
       "16 -0.655026\n",
       "17 -0.463215\n",
       "18  0.152592\n",
       "19 -0.217824\n",
       "20 -0.778586\n",
       "21 -0.527723\n",
       "22 -0.051708\n",
       "23 -0.481998\n",
       "24 -0.228224\n",
       "25 -0.118447\n",
       "26 -0.374483\n",
       "27  0.034428\n",
       "28 -0.213135\n",
       "29 -0.257388"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for batch_X, batch_Y in cartesian_product_generator(data1, data2, df_Y, batch_size1):\n",
    "    display(batch_X)\n",
    "    display(batch_Y)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Gene</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>...</th>\n",
       "      <th>1890</th>\n",
       "      <th>1891</th>\n",
       "      <th>1892</th>\n",
       "      <th>1893</th>\n",
       "      <th>1894</th>\n",
       "      <th>1895</th>\n",
       "      <th>1896</th>\n",
       "      <th>1897</th>\n",
       "      <th>1898</th>\n",
       "      <th>1899</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ZFX</td>\n",
       "      <td>0.013528</td>\n",
       "      <td>-0.073574</td>\n",
       "      <td>0.050425</td>\n",
       "      <td>-0.007168</td>\n",
       "      <td>-0.083504</td>\n",
       "      <td>0.009434</td>\n",
       "      <td>-0.243107</td>\n",
       "      <td>-0.036453</td>\n",
       "      <td>-0.003885</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005840</td>\n",
       "      <td>0.054215</td>\n",
       "      <td>-0.018552</td>\n",
       "      <td>0.010348</td>\n",
       "      <td>0.048311</td>\n",
       "      <td>0.011924</td>\n",
       "      <td>0.011584</td>\n",
       "      <td>0.030220</td>\n",
       "      <td>0.333860</td>\n",
       "      <td>0.010832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LAMP2</td>\n",
       "      <td>0.004373</td>\n",
       "      <td>-0.052026</td>\n",
       "      <td>0.115090</td>\n",
       "      <td>-0.018382</td>\n",
       "      <td>-0.366196</td>\n",
       "      <td>0.019519</td>\n",
       "      <td>-0.149446</td>\n",
       "      <td>-0.032409</td>\n",
       "      <td>-0.004365</td>\n",
       "      <td>...</td>\n",
       "      <td>0.053146</td>\n",
       "      <td>0.022796</td>\n",
       "      <td>0.041019</td>\n",
       "      <td>0.017856</td>\n",
       "      <td>-0.112012</td>\n",
       "      <td>0.043097</td>\n",
       "      <td>0.028423</td>\n",
       "      <td>0.056248</td>\n",
       "      <td>0.138608</td>\n",
       "      <td>0.031280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ITGA2B</td>\n",
       "      <td>0.005604</td>\n",
       "      <td>-0.024311</td>\n",
       "      <td>0.063571</td>\n",
       "      <td>-0.014624</td>\n",
       "      <td>-0.464599</td>\n",
       "      <td>0.016603</td>\n",
       "      <td>-0.144890</td>\n",
       "      <td>-0.017510</td>\n",
       "      <td>-0.003596</td>\n",
       "      <td>...</td>\n",
       "      <td>0.075977</td>\n",
       "      <td>-0.017172</td>\n",
       "      <td>0.020559</td>\n",
       "      <td>0.034451</td>\n",
       "      <td>-0.044476</td>\n",
       "      <td>-0.010408</td>\n",
       "      <td>0.003882</td>\n",
       "      <td>0.095413</td>\n",
       "      <td>0.050243</td>\n",
       "      <td>0.038366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ASB4</td>\n",
       "      <td>0.003966</td>\n",
       "      <td>-0.157932</td>\n",
       "      <td>0.034745</td>\n",
       "      <td>-0.003395</td>\n",
       "      <td>0.023854</td>\n",
       "      <td>-0.018922</td>\n",
       "      <td>0.011001</td>\n",
       "      <td>-0.031456</td>\n",
       "      <td>-0.005197</td>\n",
       "      <td>...</td>\n",
       "      <td>0.040247</td>\n",
       "      <td>0.077687</td>\n",
       "      <td>-0.035935</td>\n",
       "      <td>0.004659</td>\n",
       "      <td>-0.122684</td>\n",
       "      <td>-0.008640</td>\n",
       "      <td>0.038181</td>\n",
       "      <td>0.139848</td>\n",
       "      <td>-0.008251</td>\n",
       "      <td>-0.013466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>GDE1</td>\n",
       "      <td>0.013251</td>\n",
       "      <td>0.258344</td>\n",
       "      <td>0.148767</td>\n",
       "      <td>-0.011984</td>\n",
       "      <td>-0.054923</td>\n",
       "      <td>0.024582</td>\n",
       "      <td>-0.055574</td>\n",
       "      <td>-0.057606</td>\n",
       "      <td>-0.006614</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.049405</td>\n",
       "      <td>-0.035010</td>\n",
       "      <td>-0.057596</td>\n",
       "      <td>0.064666</td>\n",
       "      <td>-0.099617</td>\n",
       "      <td>-0.003899</td>\n",
       "      <td>0.068397</td>\n",
       "      <td>-0.026983</td>\n",
       "      <td>-0.023365</td>\n",
       "      <td>-0.025853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17538</th>\n",
       "      <td>TMEM247</td>\n",
       "      <td>0.007376</td>\n",
       "      <td>-0.055579</td>\n",
       "      <td>0.015968</td>\n",
       "      <td>0.000652</td>\n",
       "      <td>-0.090398</td>\n",
       "      <td>0.004820</td>\n",
       "      <td>-0.177906</td>\n",
       "      <td>-0.015425</td>\n",
       "      <td>-0.006904</td>\n",
       "      <td>...</td>\n",
       "      <td>0.040532</td>\n",
       "      <td>0.027138</td>\n",
       "      <td>-0.139576</td>\n",
       "      <td>0.042335</td>\n",
       "      <td>-0.019513</td>\n",
       "      <td>-0.029749</td>\n",
       "      <td>0.099430</td>\n",
       "      <td>-0.019195</td>\n",
       "      <td>0.082027</td>\n",
       "      <td>0.096192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17539</th>\n",
       "      <td>EEF1AKMT4</td>\n",
       "      <td>0.006579</td>\n",
       "      <td>0.054662</td>\n",
       "      <td>0.058391</td>\n",
       "      <td>-0.011576</td>\n",
       "      <td>-0.136212</td>\n",
       "      <td>-0.006775</td>\n",
       "      <td>-0.078994</td>\n",
       "      <td>-0.114747</td>\n",
       "      <td>-0.006499</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010785</td>\n",
       "      <td>0.021735</td>\n",
       "      <td>-0.020145</td>\n",
       "      <td>-0.087090</td>\n",
       "      <td>-0.070418</td>\n",
       "      <td>0.054884</td>\n",
       "      <td>0.017232</td>\n",
       "      <td>0.025832</td>\n",
       "      <td>0.109838</td>\n",
       "      <td>0.177065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17540</th>\n",
       "      <td>TBCE</td>\n",
       "      <td>0.004036</td>\n",
       "      <td>-0.087206</td>\n",
       "      <td>0.031435</td>\n",
       "      <td>-0.010328</td>\n",
       "      <td>0.106398</td>\n",
       "      <td>0.000606</td>\n",
       "      <td>-0.061067</td>\n",
       "      <td>-0.038477</td>\n",
       "      <td>-0.006378</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005424</td>\n",
       "      <td>0.037653</td>\n",
       "      <td>0.015974</td>\n",
       "      <td>0.058448</td>\n",
       "      <td>-0.102026</td>\n",
       "      <td>-0.009397</td>\n",
       "      <td>-0.068728</td>\n",
       "      <td>-0.000311</td>\n",
       "      <td>-0.100703</td>\n",
       "      <td>0.022476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17541</th>\n",
       "      <td>CCDC39</td>\n",
       "      <td>0.006182</td>\n",
       "      <td>-0.038689</td>\n",
       "      <td>0.041625</td>\n",
       "      <td>-0.007740</td>\n",
       "      <td>-0.266323</td>\n",
       "      <td>-0.044024</td>\n",
       "      <td>0.244122</td>\n",
       "      <td>-0.006107</td>\n",
       "      <td>-0.002780</td>\n",
       "      <td>...</td>\n",
       "      <td>0.068222</td>\n",
       "      <td>0.006219</td>\n",
       "      <td>0.012965</td>\n",
       "      <td>0.039683</td>\n",
       "      <td>-0.014748</td>\n",
       "      <td>-0.024827</td>\n",
       "      <td>0.035910</td>\n",
       "      <td>-0.009677</td>\n",
       "      <td>0.042979</td>\n",
       "      <td>0.058219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17542</th>\n",
       "      <td>NPBWR1</td>\n",
       "      <td>0.077194</td>\n",
       "      <td>-0.032025</td>\n",
       "      <td>0.066356</td>\n",
       "      <td>-0.011709</td>\n",
       "      <td>-0.088859</td>\n",
       "      <td>0.154841</td>\n",
       "      <td>-0.071940</td>\n",
       "      <td>-0.102432</td>\n",
       "      <td>-0.004334</td>\n",
       "      <td>...</td>\n",
       "      <td>0.039568</td>\n",
       "      <td>-0.049626</td>\n",
       "      <td>-0.026368</td>\n",
       "      <td>-0.019549</td>\n",
       "      <td>-0.131308</td>\n",
       "      <td>0.010754</td>\n",
       "      <td>0.138199</td>\n",
       "      <td>0.016953</td>\n",
       "      <td>0.008274</td>\n",
       "      <td>-0.029643</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>17543 rows Ã— 1901 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Gene         0         1         2         3         4         5  \\\n",
       "0            ZFX  0.013528 -0.073574  0.050425 -0.007168 -0.083504  0.009434   \n",
       "1          LAMP2  0.004373 -0.052026  0.115090 -0.018382 -0.366196  0.019519   \n",
       "2         ITGA2B  0.005604 -0.024311  0.063571 -0.014624 -0.464599  0.016603   \n",
       "3           ASB4  0.003966 -0.157932  0.034745 -0.003395  0.023854 -0.018922   \n",
       "4           GDE1  0.013251  0.258344  0.148767 -0.011984 -0.054923  0.024582   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "17538    TMEM247  0.007376 -0.055579  0.015968  0.000652 -0.090398  0.004820   \n",
       "17539  EEF1AKMT4  0.006579  0.054662  0.058391 -0.011576 -0.136212 -0.006775   \n",
       "17540       TBCE  0.004036 -0.087206  0.031435 -0.010328  0.106398  0.000606   \n",
       "17541     CCDC39  0.006182 -0.038689  0.041625 -0.007740 -0.266323 -0.044024   \n",
       "17542     NPBWR1  0.077194 -0.032025  0.066356 -0.011709 -0.088859  0.154841   \n",
       "\n",
       "              6         7         8  ...      1890      1891      1892  \\\n",
       "0     -0.243107 -0.036453 -0.003885  ...  0.005840  0.054215 -0.018552   \n",
       "1     -0.149446 -0.032409 -0.004365  ...  0.053146  0.022796  0.041019   \n",
       "2     -0.144890 -0.017510 -0.003596  ...  0.075977 -0.017172  0.020559   \n",
       "3      0.011001 -0.031456 -0.005197  ...  0.040247  0.077687 -0.035935   \n",
       "4     -0.055574 -0.057606 -0.006614  ... -0.049405 -0.035010 -0.057596   \n",
       "...         ...       ...       ...  ...       ...       ...       ...   \n",
       "17538 -0.177906 -0.015425 -0.006904  ...  0.040532  0.027138 -0.139576   \n",
       "17539 -0.078994 -0.114747 -0.006499  ...  0.010785  0.021735 -0.020145   \n",
       "17540 -0.061067 -0.038477 -0.006378  ...  0.005424  0.037653  0.015974   \n",
       "17541  0.244122 -0.006107 -0.002780  ...  0.068222  0.006219  0.012965   \n",
       "17542 -0.071940 -0.102432 -0.004334  ...  0.039568 -0.049626 -0.026368   \n",
       "\n",
       "           1893      1894      1895      1896      1897      1898      1899  \n",
       "0      0.010348  0.048311  0.011924  0.011584  0.030220  0.333860  0.010832  \n",
       "1      0.017856 -0.112012  0.043097  0.028423  0.056248  0.138608  0.031280  \n",
       "2      0.034451 -0.044476 -0.010408  0.003882  0.095413  0.050243  0.038366  \n",
       "3      0.004659 -0.122684 -0.008640  0.038181  0.139848 -0.008251 -0.013466  \n",
       "4      0.064666 -0.099617 -0.003899  0.068397 -0.026983 -0.023365 -0.025853  \n",
       "...         ...       ...       ...       ...       ...       ...       ...  \n",
       "17538  0.042335 -0.019513 -0.029749  0.099430 -0.019195  0.082027  0.096192  \n",
       "17539 -0.087090 -0.070418  0.054884  0.017232  0.025832  0.109838  0.177065  \n",
       "17540  0.058448 -0.102026 -0.009397 -0.068728 -0.000311 -0.100703  0.022476  \n",
       "17541  0.039683 -0.014748 -0.024827  0.035910 -0.009677  0.042979  0.058219  \n",
       "17542 -0.019549 -0.131308  0.010754  0.138199  0.016953  0.008274 -0.029643  \n",
       "\n",
       "[17543 rows x 1901 columns]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1898</th>\n",
       "      <th>1899</th>\n",
       "      <th>0_y</th>\n",
       "      <th>1_y</th>\n",
       "      <th>2_y</th>\n",
       "      <th>3_y</th>\n",
       "      <th>4_y</th>\n",
       "      <th>5_y</th>\n",
       "      <th>6_y</th>\n",
       "      <th>7_y</th>\n",
       "      <th>...</th>\n",
       "      <th>502_y</th>\n",
       "      <th>503_y</th>\n",
       "      <th>504_y</th>\n",
       "      <th>505_y</th>\n",
       "      <th>506_y</th>\n",
       "      <th>507_y</th>\n",
       "      <th>508_y</th>\n",
       "      <th>509_y</th>\n",
       "      <th>510_y</th>\n",
       "      <th>511_y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.33386</td>\n",
       "      <td>0.010832</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>0.000033</td>\n",
       "      <td>-0.000010</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000176</td>\n",
       "      <td>-0.000150</td>\n",
       "      <td>-0.000153</td>\n",
       "      <td>0.000060</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000140</td>\n",
       "      <td>0.000144</td>\n",
       "      <td>-0.000168</td>\n",
       "      <td>-0.000127</td>\n",
       "      <td>0.000486</td>\n",
       "      <td>-0.000168</td>\n",
       "      <td>0.000188</td>\n",
       "      <td>-0.000161</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>-0.000165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.33386</td>\n",
       "      <td>0.010832</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>0.000033</td>\n",
       "      <td>-0.000011</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000174</td>\n",
       "      <td>-0.000148</td>\n",
       "      <td>-0.000154</td>\n",
       "      <td>0.000062</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000141</td>\n",
       "      <td>0.000146</td>\n",
       "      <td>-0.000168</td>\n",
       "      <td>-0.000130</td>\n",
       "      <td>0.000486</td>\n",
       "      <td>-0.000167</td>\n",
       "      <td>0.000185</td>\n",
       "      <td>-0.000161</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>-0.000164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.33386</td>\n",
       "      <td>0.010832</td>\n",
       "      <td>-0.000017</td>\n",
       "      <td>-0.000032</td>\n",
       "      <td>0.000128</td>\n",
       "      <td>-0.000038</td>\n",
       "      <td>0.000098</td>\n",
       "      <td>-0.000182</td>\n",
       "      <td>0.000033</td>\n",
       "      <td>0.000178</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000005</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>0.000053</td>\n",
       "      <td>-0.000043</td>\n",
       "      <td>-0.000504</td>\n",
       "      <td>-0.000078</td>\n",
       "      <td>0.000085</td>\n",
       "      <td>-0.000107</td>\n",
       "      <td>0.000034</td>\n",
       "      <td>-0.000070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.33386</td>\n",
       "      <td>0.010832</td>\n",
       "      <td>-0.000016</td>\n",
       "      <td>-0.000033</td>\n",
       "      <td>0.000127</td>\n",
       "      <td>-0.000041</td>\n",
       "      <td>0.000099</td>\n",
       "      <td>-0.000183</td>\n",
       "      <td>0.000032</td>\n",
       "      <td>0.000179</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000006</td>\n",
       "      <td>0.000036</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>-0.000044</td>\n",
       "      <td>-0.000501</td>\n",
       "      <td>-0.000076</td>\n",
       "      <td>0.000087</td>\n",
       "      <td>-0.000109</td>\n",
       "      <td>0.000034</td>\n",
       "      <td>-0.000071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.33386</td>\n",
       "      <td>0.010832</td>\n",
       "      <td>-0.000018</td>\n",
       "      <td>-0.000031</td>\n",
       "      <td>0.000129</td>\n",
       "      <td>-0.000042</td>\n",
       "      <td>0.000096</td>\n",
       "      <td>-0.000185</td>\n",
       "      <td>0.000032</td>\n",
       "      <td>0.000180</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000004</td>\n",
       "      <td>0.000034</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>-0.000044</td>\n",
       "      <td>-0.000501</td>\n",
       "      <td>-0.000077</td>\n",
       "      <td>0.000084</td>\n",
       "      <td>-0.000108</td>\n",
       "      <td>0.000033</td>\n",
       "      <td>-0.000071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>966</th>\n",
       "      <td>0.33386</td>\n",
       "      <td>0.010832</td>\n",
       "      <td>-0.000161</td>\n",
       "      <td>0.000083</td>\n",
       "      <td>0.000215</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.000161</td>\n",
       "      <td>0.000663</td>\n",
       "      <td>-0.000133</td>\n",
       "      <td>0.000253</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000511</td>\n",
       "      <td>0.000225</td>\n",
       "      <td>-0.000158</td>\n",
       "      <td>-0.000136</td>\n",
       "      <td>0.000217</td>\n",
       "      <td>-0.000201</td>\n",
       "      <td>-0.000028</td>\n",
       "      <td>-0.000254</td>\n",
       "      <td>0.000039</td>\n",
       "      <td>-0.000173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>967</th>\n",
       "      <td>0.33386</td>\n",
       "      <td>0.010832</td>\n",
       "      <td>-0.000167</td>\n",
       "      <td>0.000080</td>\n",
       "      <td>0.000215</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.000158</td>\n",
       "      <td>0.000668</td>\n",
       "      <td>-0.000137</td>\n",
       "      <td>0.000259</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000511</td>\n",
       "      <td>0.000229</td>\n",
       "      <td>-0.000156</td>\n",
       "      <td>-0.000143</td>\n",
       "      <td>0.000218</td>\n",
       "      <td>-0.000203</td>\n",
       "      <td>-0.000038</td>\n",
       "      <td>-0.000258</td>\n",
       "      <td>0.000039</td>\n",
       "      <td>-0.000176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>968</th>\n",
       "      <td>0.33386</td>\n",
       "      <td>0.010832</td>\n",
       "      <td>-0.000164</td>\n",
       "      <td>0.000083</td>\n",
       "      <td>0.000214</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.000156</td>\n",
       "      <td>0.000669</td>\n",
       "      <td>-0.000138</td>\n",
       "      <td>0.000253</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000508</td>\n",
       "      <td>0.000228</td>\n",
       "      <td>-0.000158</td>\n",
       "      <td>-0.000139</td>\n",
       "      <td>0.000219</td>\n",
       "      <td>-0.000204</td>\n",
       "      <td>-0.000039</td>\n",
       "      <td>-0.000259</td>\n",
       "      <td>0.000039</td>\n",
       "      <td>-0.000175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>969</th>\n",
       "      <td>0.33386</td>\n",
       "      <td>0.010832</td>\n",
       "      <td>-0.000166</td>\n",
       "      <td>0.000082</td>\n",
       "      <td>0.000214</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.000159</td>\n",
       "      <td>0.000664</td>\n",
       "      <td>-0.000139</td>\n",
       "      <td>0.000255</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000507</td>\n",
       "      <td>0.000231</td>\n",
       "      <td>-0.000155</td>\n",
       "      <td>-0.000143</td>\n",
       "      <td>0.000217</td>\n",
       "      <td>-0.000202</td>\n",
       "      <td>-0.000039</td>\n",
       "      <td>-0.000259</td>\n",
       "      <td>0.000038</td>\n",
       "      <td>-0.000174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>970</th>\n",
       "      <td>0.33386</td>\n",
       "      <td>0.010832</td>\n",
       "      <td>-0.000167</td>\n",
       "      <td>0.000083</td>\n",
       "      <td>0.000214</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000157</td>\n",
       "      <td>0.000664</td>\n",
       "      <td>-0.000135</td>\n",
       "      <td>0.000251</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000508</td>\n",
       "      <td>0.000227</td>\n",
       "      <td>-0.000156</td>\n",
       "      <td>-0.000142</td>\n",
       "      <td>0.000218</td>\n",
       "      <td>-0.000201</td>\n",
       "      <td>-0.000036</td>\n",
       "      <td>-0.000259</td>\n",
       "      <td>0.000038</td>\n",
       "      <td>-0.000176</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>971 rows Ã— 514 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        1898      1899       0_y       1_y       2_y       3_y       4_y  \\\n",
       "0    0.33386  0.010832  0.000024  0.000033 -0.000010  0.000005  0.000176   \n",
       "1    0.33386  0.010832  0.000024  0.000033 -0.000011  0.000006  0.000174   \n",
       "2    0.33386  0.010832 -0.000017 -0.000032  0.000128 -0.000038  0.000098   \n",
       "3    0.33386  0.010832 -0.000016 -0.000033  0.000127 -0.000041  0.000099   \n",
       "4    0.33386  0.010832 -0.000018 -0.000031  0.000129 -0.000042  0.000096   \n",
       "..       ...       ...       ...       ...       ...       ...       ...   \n",
       "966  0.33386  0.010832 -0.000161  0.000083  0.000215  0.000011  0.000161   \n",
       "967  0.33386  0.010832 -0.000167  0.000080  0.000215  0.000015  0.000158   \n",
       "968  0.33386  0.010832 -0.000164  0.000083  0.000214  0.000011  0.000156   \n",
       "969  0.33386  0.010832 -0.000166  0.000082  0.000214  0.000013  0.000159   \n",
       "970  0.33386  0.010832 -0.000167  0.000083  0.000214  0.000014  0.000157   \n",
       "\n",
       "          5_y       6_y       7_y  ...     502_y     503_y     504_y  \\\n",
       "0   -0.000150 -0.000153  0.000060  ... -0.000140  0.000144 -0.000168   \n",
       "1   -0.000148 -0.000154  0.000062  ... -0.000141  0.000146 -0.000168   \n",
       "2   -0.000182  0.000033  0.000178  ... -0.000005  0.000037  0.000053   \n",
       "3   -0.000183  0.000032  0.000179  ... -0.000006  0.000036  0.000050   \n",
       "4   -0.000185  0.000032  0.000180  ... -0.000004  0.000034  0.000049   \n",
       "..        ...       ...       ...  ...       ...       ...       ...   \n",
       "966  0.000663 -0.000133  0.000253  ... -0.000511  0.000225 -0.000158   \n",
       "967  0.000668 -0.000137  0.000259  ... -0.000511  0.000229 -0.000156   \n",
       "968  0.000669 -0.000138  0.000253  ... -0.000508  0.000228 -0.000158   \n",
       "969  0.000664 -0.000139  0.000255  ... -0.000507  0.000231 -0.000155   \n",
       "970  0.000664 -0.000135  0.000251  ... -0.000508  0.000227 -0.000156   \n",
       "\n",
       "        505_y     506_y     507_y     508_y     509_y     510_y     511_y  \n",
       "0   -0.000127  0.000486 -0.000168  0.000188 -0.000161  0.000007 -0.000165  \n",
       "1   -0.000130  0.000486 -0.000167  0.000185 -0.000161  0.000005 -0.000164  \n",
       "2   -0.000043 -0.000504 -0.000078  0.000085 -0.000107  0.000034 -0.000070  \n",
       "3   -0.000044 -0.000501 -0.000076  0.000087 -0.000109  0.000034 -0.000071  \n",
       "4   -0.000044 -0.000501 -0.000077  0.000084 -0.000108  0.000033 -0.000071  \n",
       "..        ...       ...       ...       ...       ...       ...       ...  \n",
       "966 -0.000136  0.000217 -0.000201 -0.000028 -0.000254  0.000039 -0.000173  \n",
       "967 -0.000143  0.000218 -0.000203 -0.000038 -0.000258  0.000039 -0.000176  \n",
       "968 -0.000139  0.000219 -0.000204 -0.000039 -0.000259  0.000039 -0.000175  \n",
       "969 -0.000143  0.000217 -0.000202 -0.000039 -0.000259  0.000038 -0.000174  \n",
       "970 -0.000142  0.000218 -0.000201 -0.000036 -0.000259  0.000038 -0.000176  \n",
       "\n",
       "[971 rows x 514 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_X.iloc[:, 1898:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.271543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.567019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.504965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.590148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.281329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-0.421348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-0.107869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-0.586895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-0.442476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-0.211472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>-0.354844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>-0.154453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>-0.441073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>-0.379360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>-0.059150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>-0.227465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>-0.655026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>-0.463215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.152592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>-0.217824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>-0.778586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>-0.527723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>-0.051708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>-0.481998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>-0.228224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>-0.118447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>-0.374483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.034428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>-0.213135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>-0.257388</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Y\n",
       "0   0.271543\n",
       "1  -0.567019\n",
       "2  -0.504965\n",
       "3  -0.590148\n",
       "4  -0.281329\n",
       "5  -0.421348\n",
       "6  -0.107869\n",
       "7  -0.586895\n",
       "8  -0.442476\n",
       "9  -0.211472\n",
       "10 -0.354844\n",
       "11 -0.154453\n",
       "12 -0.441073\n",
       "13 -0.379360\n",
       "14 -0.059150\n",
       "15 -0.227465\n",
       "16 -0.655026\n",
       "17 -0.463215\n",
       "18  0.152592\n",
       "19 -0.217824\n",
       "20 -0.778586\n",
       "21 -0.527723\n",
       "22 -0.051708\n",
       "23 -0.481998\n",
       "24 -0.228224\n",
       "25 -0.118447\n",
       "26 -0.374483\n",
       "27  0.034428\n",
       "28 -0.213135\n",
       "29 -0.257388"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>503</th>\n",
       "      <th>504</th>\n",
       "      <th>505</th>\n",
       "      <th>506</th>\n",
       "      <th>507</th>\n",
       "      <th>508</th>\n",
       "      <th>509</th>\n",
       "      <th>510</th>\n",
       "      <th>511</th>\n",
       "      <th>key</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000364</td>\n",
       "      <td>0.000712</td>\n",
       "      <td>0.000426</td>\n",
       "      <td>-0.000169</td>\n",
       "      <td>0.000310</td>\n",
       "      <td>-0.004777</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>-0.000258</td>\n",
       "      <td>0.000110</td>\n",
       "      <td>0.000228</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000594</td>\n",
       "      <td>0.000299</td>\n",
       "      <td>-0.000457</td>\n",
       "      <td>-0.000006</td>\n",
       "      <td>-0.000012</td>\n",
       "      <td>-0.000443</td>\n",
       "      <td>0.000373</td>\n",
       "      <td>0.000129</td>\n",
       "      <td>-0.000948</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>-0.000139</td>\n",
       "      <td>0.000040</td>\n",
       "      <td>-0.000273</td>\n",
       "      <td>0.000309</td>\n",
       "      <td>0.000109</td>\n",
       "      <td>-0.000223</td>\n",
       "      <td>-0.000303</td>\n",
       "      <td>-0.000253</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000154</td>\n",
       "      <td>0.000114</td>\n",
       "      <td>0.000162</td>\n",
       "      <td>0.000534</td>\n",
       "      <td>0.000236</td>\n",
       "      <td>-0.000229</td>\n",
       "      <td>0.000265</td>\n",
       "      <td>-0.000047</td>\n",
       "      <td>0.000220</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.000001</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>-0.000145</td>\n",
       "      <td>0.000040</td>\n",
       "      <td>-0.000268</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>0.000110</td>\n",
       "      <td>-0.000233</td>\n",
       "      <td>-0.000301</td>\n",
       "      <td>-0.000253</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000160</td>\n",
       "      <td>0.000110</td>\n",
       "      <td>0.000163</td>\n",
       "      <td>0.000534</td>\n",
       "      <td>0.000240</td>\n",
       "      <td>-0.000230</td>\n",
       "      <td>0.000263</td>\n",
       "      <td>-0.000041</td>\n",
       "      <td>0.000224</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.000003</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>-0.000141</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>-0.000269</td>\n",
       "      <td>0.000302</td>\n",
       "      <td>0.000113</td>\n",
       "      <td>-0.000228</td>\n",
       "      <td>-0.000300</td>\n",
       "      <td>-0.000249</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000159</td>\n",
       "      <td>0.000111</td>\n",
       "      <td>0.000160</td>\n",
       "      <td>0.000535</td>\n",
       "      <td>0.000240</td>\n",
       "      <td>-0.000233</td>\n",
       "      <td>0.000262</td>\n",
       "      <td>-0.000046</td>\n",
       "      <td>0.000224</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.000008</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>-0.000138</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>-0.000269</td>\n",
       "      <td>0.000305</td>\n",
       "      <td>0.000115</td>\n",
       "      <td>-0.000230</td>\n",
       "      <td>-0.000298</td>\n",
       "      <td>-0.000252</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000161</td>\n",
       "      <td>0.000113</td>\n",
       "      <td>0.000163</td>\n",
       "      <td>0.000538</td>\n",
       "      <td>0.000240</td>\n",
       "      <td>-0.000231</td>\n",
       "      <td>0.000263</td>\n",
       "      <td>-0.000050</td>\n",
       "      <td>0.000219</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-0.000004</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>-0.000137</td>\n",
       "      <td>0.000036</td>\n",
       "      <td>-0.000270</td>\n",
       "      <td>0.000302</td>\n",
       "      <td>0.000113</td>\n",
       "      <td>-0.000231</td>\n",
       "      <td>-0.000302</td>\n",
       "      <td>-0.000251</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000161</td>\n",
       "      <td>0.000111</td>\n",
       "      <td>0.000164</td>\n",
       "      <td>0.000532</td>\n",
       "      <td>0.000239</td>\n",
       "      <td>-0.000229</td>\n",
       "      <td>0.000267</td>\n",
       "      <td>-0.000044</td>\n",
       "      <td>0.000221</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-0.000004</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>-0.000141</td>\n",
       "      <td>0.000039</td>\n",
       "      <td>-0.000271</td>\n",
       "      <td>0.000303</td>\n",
       "      <td>0.000114</td>\n",
       "      <td>-0.000230</td>\n",
       "      <td>-0.000299</td>\n",
       "      <td>-0.000252</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000159</td>\n",
       "      <td>0.000112</td>\n",
       "      <td>0.000163</td>\n",
       "      <td>0.000536</td>\n",
       "      <td>0.000239</td>\n",
       "      <td>-0.000232</td>\n",
       "      <td>0.000264</td>\n",
       "      <td>-0.000046</td>\n",
       "      <td>0.000224</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-0.000004</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>-0.000139</td>\n",
       "      <td>0.000039</td>\n",
       "      <td>-0.000270</td>\n",
       "      <td>0.000302</td>\n",
       "      <td>0.000112</td>\n",
       "      <td>-0.000228</td>\n",
       "      <td>-0.000300</td>\n",
       "      <td>-0.000249</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000159</td>\n",
       "      <td>0.000111</td>\n",
       "      <td>0.000163</td>\n",
       "      <td>0.000534</td>\n",
       "      <td>0.000235</td>\n",
       "      <td>-0.000234</td>\n",
       "      <td>0.000264</td>\n",
       "      <td>-0.000046</td>\n",
       "      <td>0.000222</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-0.000002</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>-0.000141</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>-0.000274</td>\n",
       "      <td>0.000304</td>\n",
       "      <td>0.000115</td>\n",
       "      <td>-0.000230</td>\n",
       "      <td>-0.000304</td>\n",
       "      <td>-0.000255</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000158</td>\n",
       "      <td>0.000113</td>\n",
       "      <td>0.000163</td>\n",
       "      <td>0.000537</td>\n",
       "      <td>0.000238</td>\n",
       "      <td>-0.000232</td>\n",
       "      <td>0.000265</td>\n",
       "      <td>-0.000046</td>\n",
       "      <td>0.000225</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-0.000004</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>-0.000140</td>\n",
       "      <td>0.000040</td>\n",
       "      <td>-0.000277</td>\n",
       "      <td>0.000304</td>\n",
       "      <td>0.000118</td>\n",
       "      <td>-0.000230</td>\n",
       "      <td>-0.000297</td>\n",
       "      <td>-0.000251</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000159</td>\n",
       "      <td>0.000112</td>\n",
       "      <td>0.000164</td>\n",
       "      <td>0.000536</td>\n",
       "      <td>0.000239</td>\n",
       "      <td>-0.000237</td>\n",
       "      <td>0.000266</td>\n",
       "      <td>-0.000047</td>\n",
       "      <td>0.000224</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>-0.000004</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>-0.000142</td>\n",
       "      <td>0.000038</td>\n",
       "      <td>-0.000273</td>\n",
       "      <td>0.000304</td>\n",
       "      <td>0.000114</td>\n",
       "      <td>-0.000228</td>\n",
       "      <td>-0.000299</td>\n",
       "      <td>-0.000252</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000158</td>\n",
       "      <td>0.000112</td>\n",
       "      <td>0.000162</td>\n",
       "      <td>0.000536</td>\n",
       "      <td>0.000238</td>\n",
       "      <td>-0.000233</td>\n",
       "      <td>0.000266</td>\n",
       "      <td>-0.000047</td>\n",
       "      <td>0.000221</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>-0.000002</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>-0.000143</td>\n",
       "      <td>0.000038</td>\n",
       "      <td>-0.000269</td>\n",
       "      <td>0.000302</td>\n",
       "      <td>0.000114</td>\n",
       "      <td>-0.000229</td>\n",
       "      <td>-0.000300</td>\n",
       "      <td>-0.000252</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000159</td>\n",
       "      <td>0.000111</td>\n",
       "      <td>0.000167</td>\n",
       "      <td>0.000532</td>\n",
       "      <td>0.000242</td>\n",
       "      <td>-0.000234</td>\n",
       "      <td>0.000265</td>\n",
       "      <td>-0.000046</td>\n",
       "      <td>0.000220</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>-0.000004</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>-0.000141</td>\n",
       "      <td>0.000036</td>\n",
       "      <td>-0.000271</td>\n",
       "      <td>0.000306</td>\n",
       "      <td>0.000114</td>\n",
       "      <td>-0.000226</td>\n",
       "      <td>-0.000299</td>\n",
       "      <td>-0.000249</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000158</td>\n",
       "      <td>0.000111</td>\n",
       "      <td>0.000163</td>\n",
       "      <td>0.000534</td>\n",
       "      <td>0.000239</td>\n",
       "      <td>-0.000234</td>\n",
       "      <td>0.000265</td>\n",
       "      <td>-0.000045</td>\n",
       "      <td>0.000222</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>-0.000002</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>-0.000139</td>\n",
       "      <td>0.000039</td>\n",
       "      <td>-0.000271</td>\n",
       "      <td>0.000305</td>\n",
       "      <td>0.000113</td>\n",
       "      <td>-0.000230</td>\n",
       "      <td>-0.000300</td>\n",
       "      <td>-0.000250</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000156</td>\n",
       "      <td>0.000111</td>\n",
       "      <td>0.000161</td>\n",
       "      <td>0.000534</td>\n",
       "      <td>0.000238</td>\n",
       "      <td>-0.000232</td>\n",
       "      <td>0.000266</td>\n",
       "      <td>-0.000048</td>\n",
       "      <td>0.000222</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>-0.000002</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>-0.000139</td>\n",
       "      <td>0.000039</td>\n",
       "      <td>-0.000271</td>\n",
       "      <td>0.000303</td>\n",
       "      <td>0.000115</td>\n",
       "      <td>-0.000231</td>\n",
       "      <td>-0.000300</td>\n",
       "      <td>-0.000252</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000160</td>\n",
       "      <td>0.000112</td>\n",
       "      <td>0.000165</td>\n",
       "      <td>0.000535</td>\n",
       "      <td>0.000239</td>\n",
       "      <td>-0.000229</td>\n",
       "      <td>0.000266</td>\n",
       "      <td>-0.000046</td>\n",
       "      <td>0.000223</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>-0.000003</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>-0.000138</td>\n",
       "      <td>0.000038</td>\n",
       "      <td>-0.000269</td>\n",
       "      <td>0.000302</td>\n",
       "      <td>0.000115</td>\n",
       "      <td>-0.000230</td>\n",
       "      <td>-0.000299</td>\n",
       "      <td>-0.000254</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000159</td>\n",
       "      <td>0.000113</td>\n",
       "      <td>0.000161</td>\n",
       "      <td>0.000535</td>\n",
       "      <td>0.000239</td>\n",
       "      <td>-0.000228</td>\n",
       "      <td>0.000264</td>\n",
       "      <td>-0.000048</td>\n",
       "      <td>0.000225</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.000028</td>\n",
       "      <td>0.000031</td>\n",
       "      <td>-0.000011</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000176</td>\n",
       "      <td>-0.000152</td>\n",
       "      <td>-0.000153</td>\n",
       "      <td>0.000060</td>\n",
       "      <td>0.000209</td>\n",
       "      <td>0.000172</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000144</td>\n",
       "      <td>-0.000165</td>\n",
       "      <td>-0.000127</td>\n",
       "      <td>0.000485</td>\n",
       "      <td>-0.000167</td>\n",
       "      <td>0.000190</td>\n",
       "      <td>-0.000160</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>-0.000167</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.000018</td>\n",
       "      <td>0.000036</td>\n",
       "      <td>-0.000012</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000176</td>\n",
       "      <td>-0.000148</td>\n",
       "      <td>-0.000154</td>\n",
       "      <td>0.000061</td>\n",
       "      <td>0.000207</td>\n",
       "      <td>0.000174</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000142</td>\n",
       "      <td>-0.000169</td>\n",
       "      <td>-0.000130</td>\n",
       "      <td>0.000488</td>\n",
       "      <td>-0.000166</td>\n",
       "      <td>0.000190</td>\n",
       "      <td>-0.000156</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>-0.000167</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.000025</td>\n",
       "      <td>0.000032</td>\n",
       "      <td>-0.000010</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000175</td>\n",
       "      <td>-0.000148</td>\n",
       "      <td>-0.000155</td>\n",
       "      <td>0.000059</td>\n",
       "      <td>0.000208</td>\n",
       "      <td>0.000172</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000144</td>\n",
       "      <td>-0.000168</td>\n",
       "      <td>-0.000129</td>\n",
       "      <td>0.000488</td>\n",
       "      <td>-0.000169</td>\n",
       "      <td>0.000190</td>\n",
       "      <td>-0.000159</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>-0.000166</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.000028</td>\n",
       "      <td>0.000033</td>\n",
       "      <td>-0.000011</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000175</td>\n",
       "      <td>-0.000148</td>\n",
       "      <td>-0.000153</td>\n",
       "      <td>0.000058</td>\n",
       "      <td>0.000208</td>\n",
       "      <td>0.000175</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000144</td>\n",
       "      <td>-0.000164</td>\n",
       "      <td>-0.000127</td>\n",
       "      <td>0.000487</td>\n",
       "      <td>-0.000168</td>\n",
       "      <td>0.000187</td>\n",
       "      <td>-0.000158</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>-0.000167</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.000024</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>-0.000009</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000177</td>\n",
       "      <td>-0.000149</td>\n",
       "      <td>-0.000158</td>\n",
       "      <td>0.000061</td>\n",
       "      <td>0.000209</td>\n",
       "      <td>0.000173</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000146</td>\n",
       "      <td>-0.000167</td>\n",
       "      <td>-0.000128</td>\n",
       "      <td>0.000486</td>\n",
       "      <td>-0.000168</td>\n",
       "      <td>0.000192</td>\n",
       "      <td>-0.000159</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>-0.000165</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.000022</td>\n",
       "      <td>0.000031</td>\n",
       "      <td>-0.000013</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000175</td>\n",
       "      <td>-0.000149</td>\n",
       "      <td>-0.000155</td>\n",
       "      <td>0.000062</td>\n",
       "      <td>0.000207</td>\n",
       "      <td>0.000171</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000144</td>\n",
       "      <td>-0.000170</td>\n",
       "      <td>-0.000129</td>\n",
       "      <td>0.000488</td>\n",
       "      <td>-0.000170</td>\n",
       "      <td>0.000187</td>\n",
       "      <td>-0.000160</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>-0.000163</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.000026</td>\n",
       "      <td>0.000032</td>\n",
       "      <td>-0.000009</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000176</td>\n",
       "      <td>-0.000149</td>\n",
       "      <td>-0.000157</td>\n",
       "      <td>0.000063</td>\n",
       "      <td>0.000209</td>\n",
       "      <td>0.000174</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000147</td>\n",
       "      <td>-0.000168</td>\n",
       "      <td>-0.000131</td>\n",
       "      <td>0.000489</td>\n",
       "      <td>-0.000170</td>\n",
       "      <td>0.000189</td>\n",
       "      <td>-0.000160</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>-0.000165</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.000024</td>\n",
       "      <td>0.000033</td>\n",
       "      <td>-0.000010</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000176</td>\n",
       "      <td>-0.000148</td>\n",
       "      <td>-0.000155</td>\n",
       "      <td>0.000062</td>\n",
       "      <td>0.000208</td>\n",
       "      <td>0.000176</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000147</td>\n",
       "      <td>-0.000167</td>\n",
       "      <td>-0.000131</td>\n",
       "      <td>0.000489</td>\n",
       "      <td>-0.000169</td>\n",
       "      <td>0.000187</td>\n",
       "      <td>-0.000160</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>-0.000165</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.000026</td>\n",
       "      <td>0.000033</td>\n",
       "      <td>-0.000009</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000177</td>\n",
       "      <td>-0.000149</td>\n",
       "      <td>-0.000157</td>\n",
       "      <td>0.000061</td>\n",
       "      <td>0.000209</td>\n",
       "      <td>0.000172</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000143</td>\n",
       "      <td>-0.000168</td>\n",
       "      <td>-0.000129</td>\n",
       "      <td>0.000486</td>\n",
       "      <td>-0.000169</td>\n",
       "      <td>0.000191</td>\n",
       "      <td>-0.000159</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>-0.000165</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.000027</td>\n",
       "      <td>0.000033</td>\n",
       "      <td>-0.000012</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000177</td>\n",
       "      <td>-0.000149</td>\n",
       "      <td>-0.000156</td>\n",
       "      <td>0.000060</td>\n",
       "      <td>0.000211</td>\n",
       "      <td>0.000174</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000144</td>\n",
       "      <td>-0.000165</td>\n",
       "      <td>-0.000126</td>\n",
       "      <td>0.000487</td>\n",
       "      <td>-0.000168</td>\n",
       "      <td>0.000187</td>\n",
       "      <td>-0.000160</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>-0.000166</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.000023</td>\n",
       "      <td>0.000034</td>\n",
       "      <td>-0.000010</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000175</td>\n",
       "      <td>-0.000149</td>\n",
       "      <td>-0.000156</td>\n",
       "      <td>0.000060</td>\n",
       "      <td>0.000207</td>\n",
       "      <td>0.000174</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000143</td>\n",
       "      <td>-0.000168</td>\n",
       "      <td>-0.000128</td>\n",
       "      <td>0.000489</td>\n",
       "      <td>-0.000169</td>\n",
       "      <td>0.000189</td>\n",
       "      <td>-0.000160</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>-0.000165</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.000023</td>\n",
       "      <td>0.000033</td>\n",
       "      <td>-0.000011</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000173</td>\n",
       "      <td>-0.000149</td>\n",
       "      <td>-0.000155</td>\n",
       "      <td>0.000061</td>\n",
       "      <td>0.000210</td>\n",
       "      <td>0.000173</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000143</td>\n",
       "      <td>-0.000168</td>\n",
       "      <td>-0.000130</td>\n",
       "      <td>0.000487</td>\n",
       "      <td>-0.000169</td>\n",
       "      <td>0.000187</td>\n",
       "      <td>-0.000160</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>-0.000167</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>-0.001431</td>\n",
       "      <td>-0.001998</td>\n",
       "      <td>-0.001037</td>\n",
       "      <td>0.000312</td>\n",
       "      <td>-0.000804</td>\n",
       "      <td>0.014954</td>\n",
       "      <td>-0.000044</td>\n",
       "      <td>0.000792</td>\n",
       "      <td>-0.000189</td>\n",
       "      <td>-0.000333</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.001027</td>\n",
       "      <td>-0.000914</td>\n",
       "      <td>0.000910</td>\n",
       "      <td>-0.000024</td>\n",
       "      <td>0.000067</td>\n",
       "      <td>0.001326</td>\n",
       "      <td>-0.000922</td>\n",
       "      <td>-0.000325</td>\n",
       "      <td>0.002181</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.000027</td>\n",
       "      <td>0.000033</td>\n",
       "      <td>-0.000009</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.000174</td>\n",
       "      <td>-0.000148</td>\n",
       "      <td>-0.000155</td>\n",
       "      <td>0.000059</td>\n",
       "      <td>0.000209</td>\n",
       "      <td>0.000175</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000144</td>\n",
       "      <td>-0.000169</td>\n",
       "      <td>-0.000127</td>\n",
       "      <td>0.000488</td>\n",
       "      <td>-0.000170</td>\n",
       "      <td>0.000190</td>\n",
       "      <td>-0.000159</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>-0.000165</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>30 rows Ã— 513 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           0         1         2         3         4         5         6  \\\n",
       "0   0.000364  0.000712  0.000426 -0.000169  0.000310 -0.004777  0.000029   \n",
       "1   0.000003  0.000009 -0.000139  0.000040 -0.000273  0.000309  0.000109   \n",
       "2  -0.000001  0.000017 -0.000145  0.000040 -0.000268  0.000300  0.000110   \n",
       "3  -0.000003  0.000013 -0.000141  0.000035 -0.000269  0.000302  0.000113   \n",
       "4  -0.000008  0.000018 -0.000138  0.000037 -0.000269  0.000305  0.000115   \n",
       "5  -0.000004  0.000017 -0.000137  0.000036 -0.000270  0.000302  0.000113   \n",
       "6  -0.000004  0.000014 -0.000141  0.000039 -0.000271  0.000303  0.000114   \n",
       "7  -0.000004  0.000009 -0.000139  0.000039 -0.000270  0.000302  0.000112   \n",
       "8  -0.000002  0.000011 -0.000141  0.000037 -0.000274  0.000304  0.000115   \n",
       "9  -0.000004  0.000014 -0.000140  0.000040 -0.000277  0.000304  0.000118   \n",
       "10 -0.000004  0.000012 -0.000142  0.000038 -0.000273  0.000304  0.000114   \n",
       "11 -0.000002  0.000017 -0.000143  0.000038 -0.000269  0.000302  0.000114   \n",
       "12 -0.000004  0.000016 -0.000141  0.000036 -0.000271  0.000306  0.000114   \n",
       "13 -0.000002  0.000013 -0.000139  0.000039 -0.000271  0.000305  0.000113   \n",
       "14 -0.000002  0.000014 -0.000139  0.000039 -0.000271  0.000303  0.000115   \n",
       "15 -0.000003  0.000015 -0.000138  0.000038 -0.000269  0.000302  0.000115   \n",
       "16  0.000028  0.000031 -0.000011  0.000004  0.000176 -0.000152 -0.000153   \n",
       "17  0.000018  0.000036 -0.000012  0.000007  0.000176 -0.000148 -0.000154   \n",
       "18  0.000025  0.000032 -0.000010  0.000006  0.000175 -0.000148 -0.000155   \n",
       "19  0.000028  0.000033 -0.000011  0.000005  0.000175 -0.000148 -0.000153   \n",
       "20  0.000024  0.000035 -0.000009  0.000006  0.000177 -0.000149 -0.000158   \n",
       "21  0.000022  0.000031 -0.000013  0.000006  0.000175 -0.000149 -0.000155   \n",
       "22  0.000026  0.000032 -0.000009  0.000007  0.000176 -0.000149 -0.000157   \n",
       "23  0.000024  0.000033 -0.000010  0.000007  0.000176 -0.000148 -0.000155   \n",
       "24  0.000026  0.000033 -0.000009  0.000007  0.000177 -0.000149 -0.000157   \n",
       "25  0.000027  0.000033 -0.000012  0.000006  0.000177 -0.000149 -0.000156   \n",
       "26  0.000023  0.000034 -0.000010  0.000007  0.000175 -0.000149 -0.000156   \n",
       "27  0.000023  0.000033 -0.000011  0.000007  0.000173 -0.000149 -0.000155   \n",
       "28 -0.001431 -0.001998 -0.001037  0.000312 -0.000804  0.014954 -0.000044   \n",
       "29  0.000027  0.000033 -0.000009  0.000009  0.000174 -0.000148 -0.000155   \n",
       "\n",
       "           7         8         9  ...       503       504       505       506  \\\n",
       "0  -0.000258  0.000110  0.000228  ...  0.000594  0.000299 -0.000457 -0.000006   \n",
       "1  -0.000223 -0.000303 -0.000253  ... -0.000154  0.000114  0.000162  0.000534   \n",
       "2  -0.000233 -0.000301 -0.000253  ... -0.000160  0.000110  0.000163  0.000534   \n",
       "3  -0.000228 -0.000300 -0.000249  ... -0.000159  0.000111  0.000160  0.000535   \n",
       "4  -0.000230 -0.000298 -0.000252  ... -0.000161  0.000113  0.000163  0.000538   \n",
       "5  -0.000231 -0.000302 -0.000251  ... -0.000161  0.000111  0.000164  0.000532   \n",
       "6  -0.000230 -0.000299 -0.000252  ... -0.000159  0.000112  0.000163  0.000536   \n",
       "7  -0.000228 -0.000300 -0.000249  ... -0.000159  0.000111  0.000163  0.000534   \n",
       "8  -0.000230 -0.000304 -0.000255  ... -0.000158  0.000113  0.000163  0.000537   \n",
       "9  -0.000230 -0.000297 -0.000251  ... -0.000159  0.000112  0.000164  0.000536   \n",
       "10 -0.000228 -0.000299 -0.000252  ... -0.000158  0.000112  0.000162  0.000536   \n",
       "11 -0.000229 -0.000300 -0.000252  ... -0.000159  0.000111  0.000167  0.000532   \n",
       "12 -0.000226 -0.000299 -0.000249  ... -0.000158  0.000111  0.000163  0.000534   \n",
       "13 -0.000230 -0.000300 -0.000250  ... -0.000156  0.000111  0.000161  0.000534   \n",
       "14 -0.000231 -0.000300 -0.000252  ... -0.000160  0.000112  0.000165  0.000535   \n",
       "15 -0.000230 -0.000299 -0.000254  ... -0.000159  0.000113  0.000161  0.000535   \n",
       "16  0.000060  0.000209  0.000172  ...  0.000144 -0.000165 -0.000127  0.000485   \n",
       "17  0.000061  0.000207  0.000174  ...  0.000142 -0.000169 -0.000130  0.000488   \n",
       "18  0.000059  0.000208  0.000172  ...  0.000144 -0.000168 -0.000129  0.000488   \n",
       "19  0.000058  0.000208  0.000175  ...  0.000144 -0.000164 -0.000127  0.000487   \n",
       "20  0.000061  0.000209  0.000173  ...  0.000146 -0.000167 -0.000128  0.000486   \n",
       "21  0.000062  0.000207  0.000171  ...  0.000144 -0.000170 -0.000129  0.000488   \n",
       "22  0.000063  0.000209  0.000174  ...  0.000147 -0.000168 -0.000131  0.000489   \n",
       "23  0.000062  0.000208  0.000176  ...  0.000147 -0.000167 -0.000131  0.000489   \n",
       "24  0.000061  0.000209  0.000172  ...  0.000143 -0.000168 -0.000129  0.000486   \n",
       "25  0.000060  0.000211  0.000174  ...  0.000144 -0.000165 -0.000126  0.000487   \n",
       "26  0.000060  0.000207  0.000174  ...  0.000143 -0.000168 -0.000128  0.000489   \n",
       "27  0.000061  0.000210  0.000173  ...  0.000143 -0.000168 -0.000130  0.000487   \n",
       "28  0.000792 -0.000189 -0.000333  ... -0.001027 -0.000914  0.000910 -0.000024   \n",
       "29  0.000059  0.000209  0.000175  ...  0.000144 -0.000169 -0.000127  0.000488   \n",
       "\n",
       "         507       508       509       510       511  key  \n",
       "0  -0.000012 -0.000443  0.000373  0.000129 -0.000948    1  \n",
       "1   0.000236 -0.000229  0.000265 -0.000047  0.000220    1  \n",
       "2   0.000240 -0.000230  0.000263 -0.000041  0.000224    1  \n",
       "3   0.000240 -0.000233  0.000262 -0.000046  0.000224    1  \n",
       "4   0.000240 -0.000231  0.000263 -0.000050  0.000219    1  \n",
       "5   0.000239 -0.000229  0.000267 -0.000044  0.000221    1  \n",
       "6   0.000239 -0.000232  0.000264 -0.000046  0.000224    1  \n",
       "7   0.000235 -0.000234  0.000264 -0.000046  0.000222    1  \n",
       "8   0.000238 -0.000232  0.000265 -0.000046  0.000225    1  \n",
       "9   0.000239 -0.000237  0.000266 -0.000047  0.000224    1  \n",
       "10  0.000238 -0.000233  0.000266 -0.000047  0.000221    1  \n",
       "11  0.000242 -0.000234  0.000265 -0.000046  0.000220    1  \n",
       "12  0.000239 -0.000234  0.000265 -0.000045  0.000222    1  \n",
       "13  0.000238 -0.000232  0.000266 -0.000048  0.000222    1  \n",
       "14  0.000239 -0.000229  0.000266 -0.000046  0.000223    1  \n",
       "15  0.000239 -0.000228  0.000264 -0.000048  0.000225    1  \n",
       "16 -0.000167  0.000190 -0.000160  0.000005 -0.000167    1  \n",
       "17 -0.000166  0.000190 -0.000156  0.000008 -0.000167    1  \n",
       "18 -0.000169  0.000190 -0.000159  0.000008 -0.000166    1  \n",
       "19 -0.000168  0.000187 -0.000158  0.000005 -0.000167    1  \n",
       "20 -0.000168  0.000192 -0.000159  0.000008 -0.000165    1  \n",
       "21 -0.000170  0.000187 -0.000160  0.000008 -0.000163    1  \n",
       "22 -0.000170  0.000189 -0.000160  0.000006 -0.000165    1  \n",
       "23 -0.000169  0.000187 -0.000160  0.000005 -0.000165    1  \n",
       "24 -0.000169  0.000191 -0.000159  0.000007 -0.000165    1  \n",
       "25 -0.000168  0.000187 -0.000160  0.000007 -0.000166    1  \n",
       "26 -0.000169  0.000189 -0.000160  0.000004 -0.000165    1  \n",
       "27 -0.000169  0.000187 -0.000160  0.000006 -0.000167    1  \n",
       "28  0.000067  0.001326 -0.000922 -0.000325  0.002181    1  \n",
       "29 -0.000170  0.000190 -0.000159  0.000006 -0.000165    1  \n",
       "\n",
       "[30 rows x 513 columns]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = []\n",
    "epoch = 1\n",
    "\n",
    "for minor in range(1, len(data1) + 1):\n",
    "    epoched = f'{epoch}.{minor:02}'  \n",
    "    epochs.append(float(epoched)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17543"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#a.py\n",
    "\"\"\"\n",
    "\n",
    "Goal\n",
    "Test the model on the saved models, and calculate the metrics for each gene concatinated with the cell line embeddings\n",
    "\n",
    "it calculate the correlation by each seen gene\n",
    "\"\"\"\n",
    "import torch \n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import os\n",
    "import torch.nn.functional as F\n",
    "from functions import FullConnectedBlock, NeuralNetwork2\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Initialize lists to store metrics\n",
    "\n",
    "\n",
    "avg_test_losses = []\n",
    "mae_values = []\n",
    "rmse_values = []\n",
    "r2_values = []\n",
    "correlation_coefficients = []\n",
    "p_values = []\n",
    "\n",
    "\n",
    "\n",
    "# Loop through the saved models\n",
    "# for epoch in range(num_epochs):\n",
    "\n",
    "target_epochs = [8]\n",
    "\n",
    "# Loop through the target epochs\n",
    "for epoch in target_epochs:\n",
    "    #model_epoch = epoch\n",
    "    Y_data_count = 0\n",
    "\n",
    "    model_path = f'/mnt/data/macaulay/model_state2/crispr_fc1_model_state_epoch_{epoch}.pth'\n",
    "    if os.path.exists(model_path):\n",
    "        \n",
    "\n",
    "        data1_test = pd.read_csv(\"/mnt/data/macaulay/datas/test_omicExpression_Embeddings.csv\") #30 >>\n",
    "        data2_test = pd.read_csv(\"/mnt/data/macaulay/datas/training_gene_embeddings.csv\") #17000 >>\n",
    "        df_Y_test = pd.read_csv('/mnt/data/macaulay/datas/A_test_gene__Y_crispr.csv') #>>\n",
    "        print('Test data loaded successfully')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        batch_size1 = len(data1_test)\n",
    "        batch_size_Y = len(data1_test)\n",
    "        batch_size2 = 1\n",
    "        combined_batches_test = []\n",
    "\n",
    "        input_dim = data1_test.shape[1] + data2_test.shape[1] - 1\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        print('Device:', device)\n",
    "        model = NeuralNetwork2(input_dim)\n",
    "\n",
    "        # If multiple GPUs\n",
    "        if torch.cuda.device_count() >= 4:\n",
    "            print(\"Using 4 GPUs!\")\n",
    "            model = nn.DataParallel(model, device_ids=list(range(4)))\n",
    "\n",
    "        model.to(device)\n",
    "        model.load_state_dict(torch.load(model_path))\n",
    "        model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "        # Define the loss function as Mean Squared Error\n",
    "        loss_fn = nn.MSELoss()\n",
    "\n",
    "        \n",
    "\n",
    "        for i in range(0, len(data1_test), batch_size1):\n",
    "            batch_data1_test = data1_test.iloc[i:i + batch_size1]\n",
    "            batch_data2_test = data2_test.iloc[0:len(data2_test)]\n",
    "\n",
    "\n",
    "            batch_data1_test['key'] = 1\n",
    "            batch_data2_test['key'] = 1\n",
    "            batch_data1_test = batch_data1_test[list(data1_test.columns[0:]) + ['key']]\n",
    "            batch_data2_test = batch_data2_test[list(data2_test.columns[1:]) + ['key']]\n",
    "            combined_batch = pd.merge(batch_data1_test, batch_data2_test, on='key').drop(columns=['key'])\n",
    "            combined_batches.append(combined_batch)\n",
    "\n",
    "\n",
    "            X_test = pd.concat(combined_batches)\n",
    "            Y_test = df_Y.iloc[Y_data_loader:(Y_data_loader + (len(data2_test) * batch_size1))]\n",
    "            Y_data_loader += (len(data2_test) * batch_size1)\n",
    "\n",
    "            combined_batches = []\n",
    "\n",
    "            \n",
    "            X_test1 = torch.tensor(X_test.values, dtype=torch.float32)\n",
    "            Y_test1 = torch.tensor(Y_test.values.reshape(-1, 1), dtype=torch.float32)\n",
    "            test_data = TensorDataset(X_test1, Y_test1)\n",
    "            test_dataloader = DataLoader(test_data, batch_size=128, shuffle=True)\n",
    "\n",
    "            #print('Test data preprocessed successfully')\n",
    "            ##################### The first 1 Batch which forms 100 rows after concat, are preprocessed and fed to the neural network under the same loop #####################\n",
    "\n",
    "            # Evaluate the model on the test data\n",
    "            test_loss = 0.0\n",
    "            actual_outputs = []\n",
    "            predicted_outputs = []\n",
    "            with torch.no_grad():  # Disable gradient calculation\n",
    "                for inputs, targets in test_dataloader:\n",
    "                    inputs = inputs.to(device)\n",
    "                    targets = targets.to(device)\n",
    "                    outputs = model(inputs)\n",
    "                    loss = loss_fn(outputs, targets)\n",
    "                    test_loss += loss.item()\n",
    "                    actual_outputs.extend(targets.cpu().numpy().flatten().tolist())\n",
    "                    predicted_outputs.extend(outputs.cpu().numpy().flatten().tolist())\n",
    "\n",
    "            avg_test_loss = test_loss / len(test_dataloader)\n",
    "            avg_test_losses.append(avg_test_loss)\n",
    "\n",
    "            # Calculate Pearson correlation coefficient\n",
    "            correlation_coefficient, p_value = pearsonr(actual_outputs, predicted_outputs)\n",
    "            correlation_coefficients.append(correlation_coefficient)\n",
    "            p_values.append(p_value)  # Append the p-value\n",
    "\n",
    "            mae = mean_absolute_error(actual_outputs, predicted_outputs)\n",
    "            rmse = np.sqrt(mean_squared_error(actual_outputs, predicted_outputs))\n",
    "            r2 = r2_score(actual_outputs, predicted_outputs)\n",
    "\n",
    "            # Append metrics to their respective lists\n",
    "            mae_values.append(mae)\n",
    "            rmse_values.append(rmse)\n",
    "            r2_values.append(r2)\n",
    "\n",
    "            print(f'Epoch {epoch} : gene {i+1}/{target_epochs}: Avg. test loss = {avg_test_loss:.4f}')\n",
    "            print(f'Epoch {epoch} : gene {i+1}/{target_epochs}: Correlation Coefficient = {correlation_coefficient:.4f}')\n",
    "            print(f'Epoch {epoch} : gene {i+1}/{target_epochs}: Mean Absolute Error = {mae:.4f}')\n",
    "            print(f'Epoch {epoch} : gene {i+1}/{target_epochs}: Root Mean Square Error = {rmse:.4f}')\n",
    "            print(f'Epoch {epoch} : gene {i+1}/{target_epochs}: R2 Score = {r2:.4f}')\n",
    "\n",
    "\n",
    "    else:\n",
    "        print(f'Model for epoch {epoch} not found!')\n",
    "\n",
    "\n",
    "# Create a list to store the full epoch labels\n",
    "epochs = []\n",
    "\n",
    "# Loop through the target epochs\n",
    "for major in target_epochs:\n",
    "    for minor in range(1, len(data2_test) + 1):\n",
    "        epoch = f'{major}.{minor:02}'  # Format as a string with two decimal places\n",
    "        epochs.append(float(epoch))\n",
    "\n",
    "# Create a DataFrame to hold the metrics\n",
    "metrics_df = pd.DataFrame({\n",
    "    'Epoch': epochs,\n",
    "    'Correlation_Coefficient': correlation_coefficients,\n",
    "    'P_Value': p_values,  # Include the p-values\n",
    "    'Test_Loss': avg_test_losses,\n",
    "    'MAE': mae_values,\n",
    "    'RMSE': rmse_values,\n",
    "    'R2_Score': r2_values\n",
    "})\n",
    "\n",
    "# Write the DataFrame to a CSV file\n",
    "metrics_path = f'datas/metrics/A_seen_genewise_correlation_metrics_summary_crispr_{target_epochs[0]}.csv'\n",
    "metrics_df.to_csv(metrics_path, index=False)\n",
    "print(f'Metrics saved to {metrics_path}')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import pandas as pd\n",
    "import os\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=pd.errors.SettingWithCopyWarning)\n",
    "from functions import FullConnectedBlock, NeuralNetwork2\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "# Initialization and Environment Setup\n",
    "def initialize_environment(data1_path, data2_path, df_Y_path, learning_rate):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print('Device:', device)\n",
    "    print(f'Number of available GPUs: {torch.cuda.device_count()}')\n",
    "\n",
    "    data1 = pd.read_csv(data1_path)\n",
    "    data2 = pd.read_csv(data2_path)\n",
    "    df_Y = pd.read_csv(df_Y_path)\n",
    "    print('Training data loaded successfully')\n",
    "\n",
    "    loss_fn = nn.MSELoss()\n",
    "    input_dim = data1.shape[1] + data2.shape[1] - 1\n",
    "    model = NeuralNetwork2(input_dim)\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    if torch.cuda.device_count() >= 4:\n",
    "        print(\"Using 4 GPUs!\")\n",
    "        model = nn.DataParallel(model, device_ids=list(range(4)))\n",
    "    print('Model initialized successfully')\n",
    "    \n",
    "    return device, data1, data2, df_Y, model, optimizer, loss_fn\n",
    "\n",
    "def load_test_data(data1_test_path_A, data2_test_path_A, df_Y_test_path_A,\n",
    "                    data1_test_path_B, data2_test_path_B, df_Y_test_path_B,\n",
    "                      data1_test_path_C, data2_test_path_C, df_Y_test_path_C):\n",
    "    \n",
    "    data1_test_A = pd.read_csv(data1_test_path_A)\n",
    "    data2_test_A = pd.read_csv(data2_test_path_A)\n",
    "    df_Y_test_A = pd.read_csv(df_Y_test_path_A)\n",
    "    data1_test_B = pd.read_csv(data1_test_path_B)\n",
    "    data2_test_B = pd.read_csv(data2_test_path_B)\n",
    "    df_Y_test_B = pd.read_csv(df_Y_test_path_B)\n",
    "    data1_test_C = pd.read_csv(data1_test_path_C)\n",
    "    data2_test_C = pd.read_csv(data2_test_path_C)\n",
    "    df_Y_test_C = pd.read_csv(df_Y_test_path_C)\n",
    "    print('Test data loaded successfully')\n",
    "    return data1_test_A, data2_test_A, df_Y_test_A, data1_test_B, data2_test_B, df_Y_test_B, data1_test_C, data2_test_C, df_Y_test_C\n",
    "\n",
    "\n",
    "\n",
    "def cartesian_product(data1, data2):\n",
    "    data1 = data1.iloc[:, 1:]\n",
    "    data1['key'] = 1\n",
    "    data2['key'] = 1\n",
    "    combined_data = pd.merge(data1, data2, on='key').drop(columns=['key'])\n",
    "    return combined_data\n",
    "\n",
    "def cartesian_product_generator(data1, data2, df_Y, batch_size1):\n",
    "    for i in range(0, len(data1), batch_size1):\n",
    "        start_idx = i * len(data2)\n",
    "        end_idx = (i + batch_size1) * len(data2)\n",
    "        batch_data1 = data1.iloc[i:i + batch_size1]\n",
    "        combined_data = cartesian_product(batch_data1, data2)\n",
    "        batch_Y = df_Y.iloc[start_idx:end_idx]\n",
    "        yield combined_data, batch_Y\n",
    "\n",
    "def load_model(model, epoch, model_save_path):\n",
    "    model_path = os.path.join(model_save_path, f'crispr_fc1_model_state_epoch_{epoch-1}.pth')\n",
    "    if os.path.exists(model_path):\n",
    "        model.load_state_dict(torch.load(model_path))\n",
    "        print(f'Model {epoch - 1} loaded successfully for epoch {epoch}')\n",
    "    else:\n",
    "        print('No saved model found. Training from scratch.')\n",
    "    return model\n",
    "\n",
    "# Evaluation Function\n",
    "def evaluate_model_on_test_data(model, data1_test, data2_test, df_Y_test, epoch, loss_fn, device, test_batch_size=128):\n",
    "    model.eval()\n",
    "    avg_test_losses = []\n",
    "    mae_values = []\n",
    "    rmse_values = []\n",
    "    r2_values = []\n",
    "    correlation_coefficients = []\n",
    "    p_values = []\n",
    "    gene_batches = 1\n",
    "    for batch_X, batch_Y in cartesian_product_generator(data1_test, data2_test, df_Y_test, gene_batches):\n",
    "        X_test = torch.tensor(batch_X.values, dtype=torch.float32)\n",
    "        Y_test = torch.tensor(batch_Y.values.reshape(-1, 1), dtype=torch.float32)\n",
    "        test_data = TensorDataset(X_test, Y_test)\n",
    "        test_dataloader = DataLoader(test_data, batch_size=test_batch_size, shuffle=True)\n",
    "\n",
    "        test_loss = 0.0\n",
    "        actual_outputs = []\n",
    "        predicted_outputs = []\n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in test_dataloader:\n",
    "                inputs = inputs.to(device)\n",
    "                targets = targets.to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss = loss_fn(outputs, targets)\n",
    "                test_loss += loss.item()\n",
    "                actual_outputs.extend(targets.cpu().numpy().flatten().tolist())\n",
    "                predicted_outputs.extend(outputs.cpu().numpy().flatten().tolist())\n",
    "\n",
    "        avg_test_loss = test_loss / len(test_dataloader)\n",
    "        avg_test_losses.append(avg_test_loss)\n",
    "        correlation_coefficient, p_value = pearsonr(actual_outputs, predicted_outputs)\n",
    "        correlation_coefficients.append(correlation_coefficient)\n",
    "        p_values.append(p_value)\n",
    "        mae = mean_absolute_error(actual_outputs, predicted_outputs)\n",
    "        rmse = np.sqrt(mean_squared_error(actual_outputs, predicted_outputs))\n",
    "        r2 = r2_score(actual_outputs, predicted_outputs)\n",
    "        mae_values.append(mae)\n",
    "        rmse_values.append(rmse)\n",
    "        r2_values.append(r2)\n",
    "\n",
    "    epochs = []\n",
    "    for minor in range(1, len(data1_test) + 1):\n",
    "        epoched = f'{epoch}.{minor:02}'  \n",
    "        epochs.append(float(epoched))\n",
    "\n",
    "    metrics_df = pd.DataFrame({\n",
    "        'Epoch': epochs,\n",
    "        'Correlation_Coefficient': correlation_coefficients,\n",
    "        'P_Value': p_values,\n",
    "        'Test_Loss': avg_test_losses,\n",
    "        'MAE': mae_values,\n",
    "        'RMSE': rmse_values,\n",
    "        'R2_Score': r2_values\n",
    "    })\n",
    "    return metrics_df\n",
    "\n",
    "# Main Training Loop\n",
    "def updated_main_training_loop(data1_path, data2_path, df_Y_path, \n",
    "                               data1_test_path_A, data2_test_path_A, df_Y_test_path_A, \n",
    "                               data1_test_path_B, data2_test_path_B, df_Y_test_path_B,\n",
    "                               data1_test_path_C, data2_test_path_C, df_Y_test_path_C,\n",
    "                               batch_size1, learning_rate, num_epochs, model_save_path, test_batch_size=128):\n",
    "    \n",
    "\n",
    "    device, data1, data2, df_Y, model, optimizer, loss_fn = initialize_environment(data1_path, data2_path, df_Y_path, learning_rate)\n",
    "\n",
    "        \n",
    "    saved_models = os.listdir(model_save_path)\n",
    "    epochs = [int(file.split('_')[-1].split('.')[0]) for file in saved_models if 'crispr_fc1_model_state_epoch_' in file]\n",
    "    last_epoch = max(epochs) if epochs else 0\n",
    "    start_epoch = last_epoch + 1\n",
    "    end_epoch = start_epoch + num_epochs\n",
    "    \n",
    "    for epoch in range(start_epoch, end_epoch):\n",
    "        model.train()\n",
    "        model = load_model(model, epoch, model_save_path)\n",
    "        for batch_X, batch_Y in cartesian_product_generator(data1, data2, df_Y, batch_size1):\n",
    "            X_train = torch.tensor(batch_X.values, dtype=torch.float32)\n",
    "            Y_train = torch.tensor(batch_Y.values.reshape(-1, 1), dtype=torch.float32)\n",
    "            train_data = TensorDataset(X_train, Y_train)\n",
    "            train_dataloader = DataLoader(train_data, batch_size=128, shuffle=True, num_workers=4, pin_memory=True)\n",
    "\n",
    "            train_loss = 0.0\n",
    "            for inputs, targets in train_dataloader:\n",
    "                inputs, targets = inputs.to(device), targets.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(inputs)\n",
    "                loss = loss_fn(outputs, targets)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                train_loss += loss.item()\n",
    "\n",
    "        torch.save(model.state_dict(), os.path.join(model_save_path, f'crispr_fc1_model_state_epoch_{epoch}.pth'))\n",
    "        \n",
    "        # Evaluate the model on test data after each epoch\n",
    "\n",
    "        data1_test_A, data2_test_A, df_Y_test_A, data1_test_B, data2_test_B, df_Y_test_B, data1_test_C, data2_test_C, df_Y_test_C = load_test_data(data1_test_path_A, data2_test_path_A, df_Y_test_path_A,\n",
    "                                                                                                                                                data1_test_path_B, data2_test_path_B, df_Y_test_path_B,\n",
    "                                                                                                                                                  data1_test_path_C, data2_test_path_C, df_Y_test_path_C)\n",
    "\n",
    "\n",
    "\n",
    "        metrics_df_A = evaluate_model_on_test_data(model, data1_test_A, data2_test_A, df_Y_test_A, epoch, loss_fn, device, test_batch_size)\n",
    "        metrics_path_A = os.path.join(model_save_path, f\"A_metrics_epoch_{epoch}.csv\")\n",
    "        metrics_df_A.to_csv(metrics_path_A, index=False)\n",
    "        print(f'Metrics for epoch {epoch} saved to {metrics_path_A}')\n",
    "\n",
    "        metrics_df_B = evaluate_model_on_test_data(model, data1_test_B, data2_test_B, df_Y_test_B, epoch, loss_fn, device, test_batch_size)\n",
    "        metrics_path_B = os.path.join(model_save_path, f\"B_metrics_epoch_{epoch}.csv\")\n",
    "        metrics_df_B.to_csv(metrics_path_B, index=False)\n",
    "        print(f'Metrics for epoch {epoch} saved to {metrics_path_B}')\n",
    "\n",
    "        metrics_df_C = evaluate_model_on_test_data(model, data1_test_C, data2_test_C, df_Y_test_C, epoch, loss_fn, device, test_batch_size)\n",
    "        metrics_path_C = os.path.join(model_save_path, f\"C_metrics_epoch_{epoch}.csv\")\n",
    "        metrics_df_C.to_csv(metrics_path_C, index=False)\n",
    "        print(f'Metrics for epoch {epoch} saved to {metrics_path_C}')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_main_training_loop(\n",
    "    data1_path, data2_path, df_Y_path,\n",
    "    data1_test_path, data2_test_path, df_Y_test_path,\n",
    "    batch_size1, learning_rate, num_epochs, model_save_path\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Dataframe:\n",
      "   A  B  C\n",
      "0  5  4  3\n",
      "1  2  1  4\n",
      "2  3  4  6\n",
      "3  4  2  8\n",
      "\n",
      "After Column-wise Normalization:\n",
      "          A         B         C\n",
      "0  5.666667  5.166667  2.000000\n",
      "1  2.000000  2.000000  3.000000\n",
      "2  3.000000  5.166667  4.666667\n",
      "3  4.666667  3.000000  5.666667\n",
      "\n",
      "After Row-wise Normalization:\n",
      "        A       B      C\n",
      "0  4.8750  4.1250  2.500\n",
      "1  3.3125  3.3125  4.875\n",
      "2  2.5000  4.8750  4.125\n",
      "3  4.1250  2.5000  4.875\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import qnorm\n",
    "\n",
    "# Sample dataframe\n",
    "df = pd.DataFrame({\n",
    "    'A': [5, 2, 3, 4],\n",
    "    'B': [4, 1, 4, 2],\n",
    "    'C': [3, 4, 6, 8]\n",
    "})\n",
    "\n",
    "# Normalize column-wise (default behavior, axis=1)\n",
    "df_normalized_columns = qnorm.quantile_normalize(df)\n",
    "\n",
    "# Normalize row-wise (set axis=0)\n",
    "df_normalized_rows = qnorm.quantile_normalize(df_normalized_columns.T).T\n",
    "\n",
    "print(\"Original Dataframe:\")\n",
    "print(df)\n",
    "print(\"\\nAfter Column-wise Normalization:\")\n",
    "print(df_normalized_columns)\n",
    "print(\"\\nAfter Row-wise Normalization:\")\n",
    "print(df_normalized_rows)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
